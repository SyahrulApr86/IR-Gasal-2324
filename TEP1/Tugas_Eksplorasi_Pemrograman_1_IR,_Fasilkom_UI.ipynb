{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXBRRPwDtRBs"
   },
   "source": [
    "# Tugas Eksplorasi Pemrograman 1\n",
    "Mata Kuliah: Perolehan Informasi\n",
    "\n",
    "Author: Adrianus Saga Ekakristi, Gibran Brahmanta\n",
    "\n",
    "Dalam tugas ini, anda akan mempelajari *text preprocessing* yang umum dilakukan sebagai langkah awal dalam sistem Information Retrieval. Terdapat 4 proses utama yang akan dibahas, yaitu *tokenization*, *lemmatization*, *stemming*, dan *stop words removal*.\n",
    "\n",
    "Notebook ini terdiri dari 2 bagian. Pertama, anda akan diberikan contoh code *text preprocessing* yang diterapkan untuk data dalam bahasa Indonesia. Pada bagian kedua, anda akan diminta untuk menulis code untuk melakukan pemrosesan serupa untuk data dalam bahasa Inggris.\n",
    "\n",
    "Selamat mengerjakan dan semoga bermanfaat!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIZW7xYJv4rR"
   },
   "source": [
    "## Bagian 1: Text Preprocessing untuk Bahasa Indonesia\n",
    "\n",
    "Bagian ini akan membahas mengenai contoh pemrosesan teks bahasa Indonesia. Silahkan pelajari dan coba pahami code-code dibawah ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wQFu0SSkj8G0",
    "ExecuteTime": {
     "end_time": "2023-09-12T14:22:22.713857900Z",
     "start_time": "2023-09-12T14:19:28.417013900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "The current date is: 12/09/2023 \n",
      "Enter the new date: (dd-mm-yy) \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time_id: int = time.time()\n",
    "! date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cw2S_vyRzDeL"
   },
   "source": [
    "### Preparasi, Instalasi Packages, Download Resources\n",
    "\n",
    "Beberapa packages perlu diinstall terlebih dahulu, seperti:\n",
    "- dataset (Hugginface Dataset)\n",
    "- PySastrasi (Implementasi Python untuk Sastrawi)\n",
    "- package-package lainnya sesuai kebutuhan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "X8x2hOdmB4Ei",
    "ExecuteTime": {
     "end_time": "2023-09-12T14:18:33.235575500Z",
     "start_time": "2023-09-12T14:18:25.835299300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets==2.10.1 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (2.10.1)\n",
      "Requirement already satisfied: PySastrawi==1.2.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (1.2.0)\n",
      "Requirement already satisfied: stanza==1.5.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (1.5.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets==2.10.1) (4.64.1)\n",
      "Requirement already satisfied: responses<0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets==2.10.1) (0.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets==2.10.1) (1.23.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets==2.10.1) (0.15.1)\n",
      "Requirement already satisfied: aiohttp in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets==2.10.1) (3.8.3)\n",
      "Requirement already satisfied: multiprocess in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets==2.10.1) (0.70.14)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets==2.10.1) (2.28.1)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets==2.10.1) (22.0)\n",
      "Requirement already satisfied: xxhash in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets==2.10.1) (2.0.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets==2.10.1) (2022.11.0)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets==2.10.1) (1.5.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets==2.10.1) (6.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets==2.10.1) (0.3.6)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets==2.10.1) (11.0.0)\n",
      "Requirement already satisfied: protobuf in c:\\programdata\\anaconda3\\lib\\site-packages (from stanza==1.5.0) (4.24.3)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from stanza==1.5.0) (1.12.1)\n",
      "Requirement already satisfied: emoji in c:\\programdata\\anaconda3\\lib\\site-packages (from stanza==1.5.0) (2.8.0)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from stanza==1.5.0) (1.16.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.10.1) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.10.1) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.10.1) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.10.1) (4.0.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.10.1) (2.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.10.1) (1.8.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.10.1) (1.2.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.10.1) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.10.1) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets==2.10.1) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets==2.10.1) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets==2.10.1) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets==2.10.1) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets==2.10.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets==2.10.1) (2022.7)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "20529c8339824a6580ad5fb1461506ee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-12 21:18:32 INFO: Downloading these customized packages for language: id (Indonesian)...\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| pretrain  | conll17 |\n",
      "=======================\n",
      "\n",
      "2023-09-12 21:18:32 INFO: File exists: C:\\Users\\USER\\stanza_resources\\id\\tokenize\\gsd.pt\n",
      "2023-09-12 21:18:32 INFO: File exists: C:\\Users\\USER\\stanza_resources\\id\\mwt\\gsd.pt\n",
      "2023-09-12 21:18:32 INFO: File exists: C:\\Users\\USER\\stanza_resources\\id\\pos\\gsd.pt\n",
      "2023-09-12 21:18:32 INFO: File exists: C:\\Users\\USER\\stanza_resources\\id\\lemma\\gsd.pt\n",
      "2023-09-12 21:18:33 INFO: File exists: C:\\Users\\USER\\stanza_resources\\id\\pretrain\\conll17.pt\n",
      "2023-09-12 21:18:33 INFO: Finished downloading models and saved to C:\\Users\\USER\\stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets==2.10.1 PySastrawi==1.2.0 stanza==1.5.0\n",
    "\n",
    "from typing import List, Dict, Any, Set, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "import multiprocessing\n",
    "import re\n",
    "\n",
    "import stanza\n",
    "stanza.download('id', processors = 'tokenize,mwt,pos,lemma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShF0KHIizyns"
   },
   "source": [
    "### Load Data\n",
    "\n",
    "Untuk contoh ini, akan digunakan data corpus Indonesian mC4 (Colossal Clean Crawled Corpus, Raffel et al., 2019), yaitu kumpulan hasil crawling web oleh Common Crawl yang sudah dibersihkan oleh Raffel et al., 2019 serta Allen AI, dan kemudian dilakukan filter untuk bahasa Indonesia.\n",
    "\n",
    "Load data dilakukan dengan memanfaatkan library HuggingFace Dataset. Beberapa parameter penting:\n",
    "- path = 'indonesian-nlp/mc4-id': merujuk ke https://huggingface.co/datasets/indonesian-nlp/mc4-id\n",
    "- name = 'tiny': config untuk melakukan loading data dalam jumlah tertentu (tiny)\n",
    "- split = 'X[:N%]': ambil N% split X data dari dataset yang telah diunduh (e.g. train[:5%])\n",
    "- num_proc: melakukan penarikan data secara parallel dengan jumlah process sebanyak nilai yang digunakan dalam parameternya\n",
    "\n",
    "Referensi:\n",
    "- https://huggingface.co/docs/datasets/loading\n",
    "- https://huggingface.co/docs/datasets/main/en/package_reference/loading_methods#datasets.load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1nhmFed7BgRi",
    "ExecuteTime": {
     "end_time": "2023-09-12T14:22:44.778230800Z",
     "start_time": "2023-09-12T14:22:39.657461800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset mc4-id (C:/Users/USER/.cache/huggingface/datasets/indonesian-nlp___mc4-id/tiny/1.0.0/721475d1d1a512521de99189896104190f5ef3c5dbbdd511442097b116178061)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "dataset_id: Dataset = load_dataset(\n",
    "    'indonesian-nlp/mc4-id',\n",
    "    'tiny',\n",
    "    split = 'validation[:5%]',\n",
    "    num_proc = min(8 * multiprocessing.cpu_count(), 60)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUY029ny6V-d"
   },
   "source": [
    "Setelah data selesai ditarik sebagai variable `dataset_id`, kita dapat melihat lebih detail dataset ini beserta dengan sample salah satu row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ubrlvXC3CB87",
    "ExecuteTime": {
     "end_time": "2023-09-12T14:22:44.851484600Z",
     "start_time": "2023-09-12T14:22:44.779800500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset variable data type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "dataset: Dataset({\n",
      "    features: ['text', 'timestamp', 'url'],\n",
      "    num_rows: 803\n",
      "})\n",
      "dataset sample: {\n",
      "  \"text\": \"03 Desember 2018, 11: 55: 59 WIB | editor : Perdana\\nPERAWATAN naskah kuno berusia ratusan tahun koleksi Rekso Pustoko memerlukan biaya tak sedikit biaya. Dirinya tak menyangkal bahwa perawatan untuk buku-buku berusia sekitar 200 tahun itu terbilang kurang. Besarnya biaya perawatan masih menjadi alasan utama sebagian besar koleksi belum tersentuh perawatan layaknya naskah dan manuskrip kuno di museum-museum atau perpustakaan besar lainnya.\\nSalah satu perawatan yang masih belum baik adalah fumigasi. Hingga saat ini, perawatan untuk membunuh biota yang merusak arsip hanya bisa dilakukan setahun sekali untuk satu ruang. Idealnya fumigasi dilakukan dua kali dalam setahun.\\nSelain fumigasi, untuk menjaga ketahanan kertas agar tidak dimakan ngengat bisa dilakukan dengan melakukan enkapsulasi dan alih media. Biaya enkapsulasi sendiri cukup besar. Satu lembar enkapsulasi dihargai sebesar Rp 20 ribu. Dalam sebulan ekapsulasi bisa dilakukan pada 1.000 lembar manuskrip sehingga biayanya sekitar 20 juta. Karena bahan yang cukup mahal serta harus ditangani profesional maka prosesnya cukup memakan waktu.\\n\\u201cMasalahnya pembiayaan masih dilakukan secara mandiri. Belum lagi minimnya tenaga ahli di sini dalam perawatan,\\u201d ujar Darweni, petugas alih aksara Rekso Pustoko .\\nBiaya untuk perawatan sendiri mengandalkan bantuan Yayasan Suryosumirat dan bantuan dari pihak luar maupun masyarakat. Pihak keraton belum bisa memberikan dana yang cukup untuk bisa merawat arsip kuno dengan baik.\\n\\u201cKarena kendala dana itu, proses alih media dengan digitalisasi juga belum selesai. Padahal ini sangat membantu agar isi teks dapat diteliti tanpa membuka naskah yang sudah tua itu,\\u201d kata Darweni.\\nKarena perawatan belum standar, sejauh ini pihaknya hanya bisa melakukan perawatan harian dengan lebih detail. Caranya dengan membubuhkan kapur barus serta rajin membersihkan dari debu. Sedikitnya 4 kg kapur barus dihabiskan untuk satu ruangan itu dan wajib selalu diganti selama beberapa hari terakhir.\\n\\u201cDi sini udaranya terlalu lembab, sehingga jamur dan kutu mudah merusak buku. Karenanya dalam perawatannya kami menggunakan sirio black biar PH nya netral. Kalau pengawetnya kami hanya menggunakan kapur barus saja. Seharusnya dipasang AC, agar suhunya bisa terjaga. Idealnya suhu tetap terjaga pada 18 derajat celsius selama 24 jam,\\u201d kata Darweni.\\nSekadar informasi, kawasan Keraton Pura Mangkunegaran menjadi salah satu Benda Cagar Budaya sejak 2013 silam. Sesuai dengan Keputusan Wali Kota Surakarta No. 646/1-2/1/2013 tentang Penetapan Bangunan dan Kawasan Kuno Bersejarah di Surakarta. Karena itu ia berharap ada perhatian lebih dari pemerintah terutama dalam menjaga keutuhan manuskrip-manuskrip penting tersebut.\\n\\u201cBudaya membaca arsip dan buku kuno menjadi penting untuk membuka mata generasi baru, bahwa sejarah dahulu yang membawa kita menjadi seperti saat ini. oleh sebab itu kami ingin pemerintah lebih perhatian lagi terhadap manuskrip kuno yang ada di sini,\\u201d ujar Darweni. (mg4/ves/bun)\",\n",
      "  \"timestamp\": \"2019-11-17T06:53:27Z\",\n",
      "  \"url\": \"https://radarsolo.jawapos.com/read/2018/12/03/106544/perawatan-masih-ala-kadarnya-bertahap-mulai-digitalisasi\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(f\"dataset variable data type: {type(dataset_id)}\")\n",
    "print(f\"dataset: {dataset_id}\")\n",
    "print(f\"dataset sample: {json.dumps(dataset_id[0], indent = 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LihZDZOd6538"
   },
   "source": [
    "Dapat dilihat bahwa terdapat beberapa kolom/field (title, text, domain, dsb) dalam dataset ini. Dalam contoh ini, kita hanya memerlukan kolom text. Untuk membuang kolom lain, dapat digunakan function remove_columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rTbAUUg_3LY0",
    "ExecuteTime": {
     "end_time": "2023-09-12T14:22:44.866624200Z",
     "start_time": "2023-09-12T14:22:44.805911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 803\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset_id = dataset_id.remove_columns([\"timestamp\", \"url\"])\n",
    "print(f\"dataset: {dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b662Twc7KpW"
   },
   "source": [
    "Untuk keperluan ilustrasi input dan output dari masing-masing langkah nantinya, kita dapat mengambil salah satu konten artikel dari dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "t2ZkoPPND2wk",
    "ExecuteTime": {
     "end_time": "2023-09-12T14:22:44.957606600Z",
     "start_time": "2023-09-12T14:22:44.894229300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example passage: 03 Desember 2018, 11: 55: 59 WIB | editor : Perdana\n",
      "PERAWATAN naskah kuno berusia ratusan tahun koleksi Rekso Pustoko memerlukan biaya tak sedikit biaya. Dirinya tak menyangkal bahwa perawatan untuk buku-buku berusia sekitar 200 tahun itu terbilang kurang. Besarnya biaya perawatan masih menjadi alasan utama sebagian besar koleksi belum tersentuh perawatan layaknya naskah dan manuskrip kuno di museum-museum atau perpustakaan besar lainnya.\n",
      "Salah satu perawatan yang masih belum baik adalah fumigasi. Hingga saat ini, perawatan untuk membunuh biota yang merusak arsip hanya bisa dilakukan setahun sekali untuk satu ruang. Idealnya fumigasi dilakukan dua kali dalam setahun.\n",
      "Selain fumigasi, untuk menjaga ketahanan kertas agar tidak dimakan ngengat bisa dilakukan dengan melakukan enkapsulasi dan alih media. Biaya enkapsulasi sendiri cukup besar. Satu lembar enkapsulasi dihargai sebesar Rp 20 ribu. Dalam sebulan ekapsulasi bisa dilakukan pada 1.000 lembar manuskrip sehingga biayanya sekitar 20 juta. Karena bahan yang cukup mahal serta harus ditangani profesional maka prosesnya cukup memakan waktu.\n",
      "“Masalahnya pembiayaan masih dilakukan secara mandiri. Belum lagi minimnya tenaga ahli di sini dalam perawatan,” ujar Darweni, petugas alih aksara Rekso Pustoko .\n",
      "Biaya untuk perawatan sendiri mengandalkan bantuan Yayasan Suryosumirat dan bantuan dari pihak luar maupun masyarakat. Pihak keraton belum bisa memberikan dana yang cukup untuk bisa merawat arsip kuno dengan baik.\n",
      "“Karena kendala dana itu, proses alih media dengan digitalisasi juga belum selesai. Padahal ini sangat membantu agar isi teks dapat diteliti tanpa membuka naskah yang sudah tua itu,” kata Darweni.\n",
      "Karena perawatan belum standar, sejauh ini pihaknya hanya bisa melakukan perawatan harian dengan lebih detail. Caranya dengan membubuhkan kapur barus serta rajin membersihkan dari debu. Sedikitnya 4 kg kapur barus dihabiskan untuk satu ruangan itu dan wajib selalu diganti selama beberapa hari terakhir.\n",
      "“Di sini udaranya terlalu lembab, sehingga jamur dan kutu mudah merusak buku. Karenanya dalam perawatannya kami menggunakan sirio black biar PH nya netral. Kalau pengawetnya kami hanya menggunakan kapur barus saja. Seharusnya dipasang AC, agar suhunya bisa terjaga. Idealnya suhu tetap terjaga pada 18 derajat celsius selama 24 jam,” kata Darweni.\n",
      "Sekadar informasi, kawasan Keraton Pura Mangkunegaran menjadi salah satu Benda Cagar Budaya sejak 2013 silam. Sesuai dengan Keputusan Wali Kota Surakarta No. 646/1-2/1/2013 tentang Penetapan Bangunan dan Kawasan Kuno Bersejarah di Surakarta. Karena itu ia berharap ada perhatian lebih dari pemerintah terutama dalam menjaga keutuhan manuskrip-manuskrip penting tersebut.\n",
      "“Budaya membaca arsip dan buku kuno menjadi penting untuk membuka mata generasi baru, bahwa sejarah dahulu yang membawa kita menjadi seperti saat ini. oleh sebab itu kami ingin pemerintah lebih perhatian lagi terhadap manuskrip kuno yang ada di sini,” ujar Darweni. (mg4/ves/bun)\n"
     ]
    }
   ],
   "source": [
    "example_passage_id: str = dataset_id[0]['text']\n",
    "print(f\"example passage: {example_passage_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvwLIbwS7mIP"
   },
   "source": [
    "### 1.1: Tokenization\n",
    "\n",
    "Tokenisasi merupakan proses mengubah teks (input) menjadi serangkaian token (output). Token adalah komponen terkecil dari teks, seperti kata, tanda baca, dan angka, dan berbagai variasi lainnya.\n",
    "\n",
    "Terdapat berbagai cara untuk melakukan tokenisasi, mulai dari penggunaan spasi sebagai delimiter, regular expression (regex), hingga metode berbasis machine learning. Dalam contoh ini, kita akan menggunakan regex untuk melakukan tokenisasi.\n",
    "\n",
    "Referensi:\n",
    "- https://docs.python.org/3/howto/regex.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tj3fx8o6_TD7",
    "ExecuteTime": {
     "end_time": "2023-09-12T14:22:49.949027400Z",
     "start_time": "2023-09-12T14:22:49.933044200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example of tokenized text: ['03', 'Desember', '2018', '11', '55', '59', 'WIB', 'editor', 'Perdana', 'PERAWATAN', 'naskah', 'kuno', 'berusia', 'ratusan', 'tahun', 'koleksi', 'Rekso', 'Pustoko', 'memerlukan', 'biaya', 'tak', 'sedikit', 'biaya', 'Dirinya', 'tak', 'menyangkal', 'bahwa', 'perawatan', 'untuk', 'buku', 'buku', 'berusia', 'sekitar', '200', 'tahun', 'itu', 'terbilang', 'kurang', 'Besarnya', 'biaya', 'perawatan', 'masih', 'menjadi', 'alasan', 'utama', 'sebagian', 'besar', 'koleksi', 'belum', 'tersentuh', 'perawatan', 'layaknya', 'naskah', 'dan', 'manuskrip', 'kuno', 'di', 'museum', 'museum', 'atau', 'perpustakaan', 'besar', 'lainnya', 'Salah', 'satu', 'perawatan', 'yang', 'masih', 'belum', 'baik', 'adalah', 'fumigasi', 'Hingga', 'saat', 'ini', 'perawatan', 'untuk', 'membunuh', 'biota', 'yang', 'merusak', 'arsip', 'hanya', 'bisa', 'dilakukan', 'setahun', 'sekali', 'untuk', 'satu', 'ruang', 'Idealnya', 'fumigasi', 'dilakukan', 'dua', 'kali', 'dalam', 'setahun', 'Selain', 'fumigasi', 'untuk', 'menjaga', 'ketahanan', 'kertas', 'agar', 'tidak', 'dimakan', 'ngengat', 'bisa', 'dilakukan', 'dengan', 'melakukan', 'enkapsulasi', 'dan', 'alih', 'media', 'Biaya', 'enkapsulasi', 'sendiri', 'cukup', 'besar', 'Satu', 'lembar', 'enkapsulasi', 'dihargai', 'sebesar', 'Rp', '20', 'ribu', 'Dalam', 'sebulan', 'ekapsulasi', 'bisa', 'dilakukan', 'pada', '1', '000', 'lembar', 'manuskrip', 'sehingga', 'biayanya', 'sekitar', '20', 'juta', 'Karena', 'bahan', 'yang', 'cukup', 'mahal', 'serta', 'harus', 'ditangani', 'profesional', 'maka', 'prosesnya', 'cukup', 'memakan', 'waktu', 'Masalahnya', 'pembiayaan', 'masih', 'dilakukan', 'secara', 'mandiri', 'Belum', 'lagi', 'minimnya', 'tenaga', 'ahli', 'di', 'sini', 'dalam', 'perawatan', 'ujar', 'Darweni', 'petugas', 'alih', 'aksara', 'Rekso', 'Pustoko', 'Biaya', 'untuk', 'perawatan', 'sendiri', 'mengandalkan', 'bantuan', 'Yayasan', 'Suryosumirat', 'dan', 'bantuan', 'dari', 'pihak', 'luar', 'maupun', 'masyarakat', 'Pihak', 'keraton', 'belum', 'bisa', 'memberikan', 'dana', 'yang', 'cukup', 'untuk', 'bisa', 'merawat', 'arsip', 'kuno', 'dengan', 'baik', 'Karena', 'kendala', 'dana', 'itu', 'proses', 'alih', 'media', 'dengan', 'digitalisasi', 'juga', 'belum', 'selesai', 'Padahal', 'ini', 'sangat', 'membantu', 'agar', 'isi', 'teks', 'dapat', 'diteliti', 'tanpa', 'membuka', 'naskah', 'yang', 'sudah', 'tua', 'itu', 'kata', 'Darweni', 'Karena', 'perawatan', 'belum', 'standar', 'sejauh', 'ini', 'pihaknya', 'hanya', 'bisa', 'melakukan', 'perawatan', 'harian', 'dengan', 'lebih', 'detail', 'Caranya', 'dengan', 'membubuhkan', 'kapur', 'barus', 'serta', 'rajin', 'membersihkan', 'dari', 'debu', 'Sedikitnya', '4', 'kg', 'kapur', 'barus', 'dihabiskan', 'untuk', 'satu', 'ruangan', 'itu', 'dan', 'wajib', 'selalu', 'diganti', 'selama', 'beberapa', 'hari', 'terakhir', 'Di', 'sini', 'udaranya', 'terlalu', 'lembab', 'sehingga', 'jamur', 'dan', 'kutu', 'mudah', 'merusak', 'buku', 'Karenanya', 'dalam', 'perawatannya', 'kami', 'menggunakan', 'sirio', 'black', 'biar', 'PH', 'nya', 'netral', 'Kalau', 'pengawetnya', 'kami', 'hanya', 'menggunakan', 'kapur', 'barus', 'saja', 'Seharusnya', 'dipasang', 'AC', 'agar', 'suhunya', 'bisa', 'terjaga', 'Idealnya', 'suhu', 'tetap', 'terjaga', 'pada', '18', 'derajat', 'celsius', 'selama', '24', 'jam', 'kata', 'Darweni', 'Sekadar', 'informasi', 'kawasan', 'Keraton', 'Pura', 'Mangkunegaran', 'menjadi', 'salah', 'satu', 'Benda', 'Cagar', 'Budaya', 'sejak', '2013', 'silam', 'Sesuai', 'dengan', 'Keputusan', 'Wali', 'Kota', 'Surakarta', 'No', '646', '1', '2', '1', '2013', 'tentang', 'Penetapan', 'Bangunan', 'dan', 'Kawasan', 'Kuno', 'Bersejarah', 'di', 'Surakarta', 'Karena', 'itu', 'ia', 'berharap', 'ada', 'perhatian', 'lebih', 'dari', 'pemerintah', 'terutama', 'dalam', 'menjaga', 'keutuhan', 'manuskrip', 'manuskrip', 'penting', 'tersebut', 'Budaya', 'membaca', 'arsip', 'dan', 'buku', 'kuno', 'menjadi', 'penting', 'untuk', 'membuka', 'mata', 'generasi', 'baru', 'bahwa', 'sejarah', 'dahulu', 'yang', 'membawa', 'kita', 'menjadi', 'seperti', 'saat', 'ini', 'oleh', 'sebab', 'itu', 'kami', 'ingin', 'pemerintah', 'lebih', 'perhatian', 'lagi', 'terhadap', 'manuskrip', 'kuno', 'yang', 'ada', 'di', 'sini', 'ujar', 'Darweni', 'mg4', 'ves', 'bun']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "tokenizer_pattern: str = r'\\w+'\n",
    "def tokenize_text_id(text: str, tokenizer_pattern: str) -> List[str]:\n",
    "  tokens: List[str] = re.findall(tokenizer_pattern, text)\n",
    "  return tokens\n",
    "\n",
    "example_tokens: List[str] = tokenize_text_id(\n",
    "  text = example_passage_id,\n",
    "  tokenizer_pattern = tokenizer_pattern,\n",
    ")\n",
    "print(f\"example of tokenized text: {example_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7E3H5P0j_5eU"
   },
   "source": [
    "### 1.2: Lemmatization\n",
    "\n",
    "Lematisasi merupakan proses untuk mencari bentuk paling dasar dari suatu token dengan memanfaatkan dictionary / mapping dari varian kata (inflected form) menjadi bentuk dasar yang disebut lemma.\n",
    "\n",
    "Contoh bahasa Indonesia: \"menyapu\", \"disapu\", \"tersapu\" -> \"sapu\"\n",
    "\n",
    "Contoh bahasa Inggris: \"studies\", \"studying\" -> \"study\"\n",
    "\n",
    "Dalam contoh code ini, akan digunakan library [Stanza](https://stanfordnlp.github.io/stanza/), sebuah library natural language processing (NLP) dari Stanford University NLP Group.\n",
    "\n",
    "Referensi:\n",
    "- https://stanfordnlp.github.io/stanza/lemma.html\n",
    "- https://stanfordnlp.github.io/stanza/tokenize.html#start-with-pretokenized-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jz0hFxM5wkJU",
    "ExecuteTime": {
     "end_time": "2023-09-12T14:22:53.315883Z",
     "start_time": "2023-09-12T14:22:52.693789700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-12 21:22:52 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a5c5c7130314a0ea892322f4a9300ef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-12 21:22:52 INFO: Loading these models for language: id (Indonesian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "=======================\n",
      "\n",
      "2023-09-12 21:22:52 INFO: Using device: cpu\n",
      "2023-09-12 21:22:52 INFO: Loading: tokenize\n",
      "2023-09-12 21:22:52 INFO: Loading: mwt\n",
      "2023-09-12 21:22:52 INFO: Loading: lemma\n",
      "2023-09-12 21:22:52 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example of tokens before lemmatization: ['03', 'Desember', '2018', '11', '55', '59', 'WIB', 'editor', 'Perdana', 'PERAWATAN', 'naskah', 'kuno', 'berusia', 'ratusan', 'tahun', 'koleksi', 'Rekso', 'Pustoko', 'memerlukan', 'biaya', 'tak', 'sedikit', 'biaya', 'Dirinya', 'tak', 'menyangkal', 'bahwa', 'perawatan', 'untuk', 'buku', 'buku', 'berusia', 'sekitar', '200', 'tahun', 'itu', 'terbilang', 'kurang', 'Besarnya', 'biaya', 'perawatan', 'masih', 'menjadi', 'alasan', 'utama', 'sebagian', 'besar', 'koleksi', 'belum', 'tersentuh', 'perawatan', 'layaknya', 'naskah', 'dan', 'manuskrip', 'kuno', 'di', 'museum', 'museum', 'atau', 'perpustakaan', 'besar', 'lainnya', 'Salah', 'satu', 'perawatan', 'yang', 'masih', 'belum', 'baik', 'adalah', 'fumigasi', 'Hingga', 'saat', 'ini', 'perawatan', 'untuk', 'membunuh', 'biota', 'yang', 'merusak', 'arsip', 'hanya', 'bisa', 'dilakukan', 'setahun', 'sekali', 'untuk', 'satu', 'ruang', 'Idealnya', 'fumigasi', 'dilakukan', 'dua', 'kali', 'dalam', 'setahun', 'Selain', 'fumigasi', 'untuk', 'menjaga', 'ketahanan', 'kertas', 'agar', 'tidak', 'dimakan', 'ngengat', 'bisa', 'dilakukan', 'dengan', 'melakukan', 'enkapsulasi', 'dan', 'alih', 'media', 'Biaya', 'enkapsulasi', 'sendiri', 'cukup', 'besar', 'Satu', 'lembar', 'enkapsulasi', 'dihargai', 'sebesar', 'Rp', '20', 'ribu', 'Dalam', 'sebulan', 'ekapsulasi', 'bisa', 'dilakukan', 'pada', '1', '000', 'lembar', 'manuskrip', 'sehingga', 'biayanya', 'sekitar', '20', 'juta', 'Karena', 'bahan', 'yang', 'cukup', 'mahal', 'serta', 'harus', 'ditangani', 'profesional', 'maka', 'prosesnya', 'cukup', 'memakan', 'waktu', 'Masalahnya', 'pembiayaan', 'masih', 'dilakukan', 'secara', 'mandiri', 'Belum', 'lagi', 'minimnya', 'tenaga', 'ahli', 'di', 'sini', 'dalam', 'perawatan', 'ujar', 'Darweni', 'petugas', 'alih', 'aksara', 'Rekso', 'Pustoko', 'Biaya', 'untuk', 'perawatan', 'sendiri', 'mengandalkan', 'bantuan', 'Yayasan', 'Suryosumirat', 'dan', 'bantuan', 'dari', 'pihak', 'luar', 'maupun', 'masyarakat', 'Pihak', 'keraton', 'belum', 'bisa', 'memberikan', 'dana', 'yang', 'cukup', 'untuk', 'bisa', 'merawat', 'arsip', 'kuno', 'dengan', 'baik', 'Karena', 'kendala', 'dana', 'itu', 'proses', 'alih', 'media', 'dengan', 'digitalisasi', 'juga', 'belum', 'selesai', 'Padahal', 'ini', 'sangat', 'membantu', 'agar', 'isi', 'teks', 'dapat', 'diteliti', 'tanpa', 'membuka', 'naskah', 'yang', 'sudah', 'tua', 'itu', 'kata', 'Darweni', 'Karena', 'perawatan', 'belum', 'standar', 'sejauh', 'ini', 'pihaknya', 'hanya', 'bisa', 'melakukan', 'perawatan', 'harian', 'dengan', 'lebih', 'detail', 'Caranya', 'dengan', 'membubuhkan', 'kapur', 'barus', 'serta', 'rajin', 'membersihkan', 'dari', 'debu', 'Sedikitnya', '4', 'kg', 'kapur', 'barus', 'dihabiskan', 'untuk', 'satu', 'ruangan', 'itu', 'dan', 'wajib', 'selalu', 'diganti', 'selama', 'beberapa', 'hari', 'terakhir', 'Di', 'sini', 'udaranya', 'terlalu', 'lembab', 'sehingga', 'jamur', 'dan', 'kutu', 'mudah', 'merusak', 'buku', 'Karenanya', 'dalam', 'perawatannya', 'kami', 'menggunakan', 'sirio', 'black', 'biar', 'PH', 'nya', 'netral', 'Kalau', 'pengawetnya', 'kami', 'hanya', 'menggunakan', 'kapur', 'barus', 'saja', 'Seharusnya', 'dipasang', 'AC', 'agar', 'suhunya', 'bisa', 'terjaga', 'Idealnya', 'suhu', 'tetap', 'terjaga', 'pada', '18', 'derajat', 'celsius', 'selama', '24', 'jam', 'kata', 'Darweni', 'Sekadar', 'informasi', 'kawasan', 'Keraton', 'Pura', 'Mangkunegaran', 'menjadi', 'salah', 'satu', 'Benda', 'Cagar', 'Budaya', 'sejak', '2013', 'silam', 'Sesuai', 'dengan', 'Keputusan', 'Wali', 'Kota', 'Surakarta', 'No', '646', '1', '2', '1', '2013', 'tentang', 'Penetapan', 'Bangunan', 'dan', 'Kawasan', 'Kuno', 'Bersejarah', 'di', 'Surakarta', 'Karena', 'itu', 'ia', 'berharap', 'ada', 'perhatian', 'lebih', 'dari', 'pemerintah', 'terutama', 'dalam', 'menjaga', 'keutuhan', 'manuskrip', 'manuskrip', 'penting', 'tersebut', 'Budaya', 'membaca', 'arsip', 'dan', 'buku', 'kuno', 'menjadi', 'penting', 'untuk', 'membuka', 'mata', 'generasi', 'baru', 'bahwa', 'sejarah', 'dahulu', 'yang', 'membawa', 'kita', 'menjadi', 'seperti', 'saat', 'ini', 'oleh', 'sebab', 'itu', 'kami', 'ingin', 'pemerintah', 'lebih', 'perhatian', 'lagi', 'terhadap', 'manuskrip', 'kuno', 'yang', 'ada', 'di', 'sini', 'ujar', 'Darweni', 'mg4', 'ves', 'bun']\n",
      "example of tokens after lemmatization:  ['03', 'desember', '2018', '11', '55', '59', 'wib', 'editor', 'perdana', 'rawat', 'naskah', 'kuno', 'usia', 'ratus', 'tahun', 'koleksi', 'rekso', 'pustoko', 'perlu', 'biaya', 'tak', 'sedikit', 'biaya', 'dirinya', 'tak', 'sangkal', 'bahwa', 'rawat', 'untuk', 'buku', 'buku', 'usia', 'sekitar', '200', 'tahun', 'itu', 'bilang', 'kurang', 'besar', 'biaya', 'rawat', 'masih', 'jadi', 'alasan', 'utama', 'sebagian', 'besar', 'koleksi', 'belum', 'tersentuh', 'rawat', 'layak', 'naskah', 'dan', 'manuskrip', 'kuno', 'di', 'museum', 'museum', 'atau', 'perpustakaan', 'besar', 'lainnya', 'salah', 'satu', 'rawat', 'yang', 'masih', 'belum', 'baik', 'adalah', 'fumigasi', 'hingga', 'saat', 'ini', 'rawat', 'untuk', 'bunuh', 'biota', 'yang', 'rusak', 'arsip', 'hanya', 'bisa', 'laku', 'setahun', 'sekali', 'untuk', 'satu', 'ruang', 'idealnya', 'fumigasi', 'laku', 'dua', 'kali', 'dalam', 'setahun', 'selain', 'fumigasi', 'untuk', 'jaga', 'tahan', 'kertas', 'agar', 'tidak', 'dimakan', 'ngengat', 'bisa', 'laku', 'dengan', 'laku', 'enkapsulasi', 'dan', 'alih', 'media', 'biaya', 'enkapsulasi', 'sendiri', 'cukup', 'besar', 'satu', 'lembar', 'enkapsulasi', 'dihargai', 'sebesar', 'rp', '20', 'ribu', 'dalam', 'sebul', 'ekapsulasi', 'bisa', 'laku', 'pada', '1', '000', 'lembar', 'manuskrip', 'sehingga', 'biayanya', 'sekitar', '20', 'juta', 'karena', 'bahan', 'yang', 'cukup', 'mahal', 'serta', 'harus', 'ditangani', 'profesional', 'maka', 'prosesnya', 'cukup', 'makan', 'waktu', 'masalahnya', 'biaya', 'masih', 'laku', 'secara', 'mandiri', 'belum', 'lagi', 'minimnya', 'tenaga', 'ahli', 'di', 'sini', 'dalam', 'rawat', 'ujar', 'darweni', 'petugas', 'alih', 'aksara', 'rekso', 'pustoko', 'biaya', 'untuk', 'rawat', 'sendiri', 'andal', 'bantu', 'yayasan', 'suryosumirat', 'dan', 'bantu', 'dari', 'pihak', 'luar', 'maupun', 'masyarakat', 'pihak', 'keraton', 'belum', 'bisa', 'beri', 'dana', 'yang', 'cukup', 'untuk', 'bisa', 'rawat', 'arsip', 'kuno', 'dengan', 'baik', 'karena', 'kendala', 'dana', 'itu', 'proses', 'alih', 'media', 'dengan', 'digitalisasi', 'juga', 'belum', 'selesai', 'padahal', 'ini', 'sangat', 'bantu', 'agar', 'isi', 'teks', 'dapat', 'teliti', 'tanpa', 'buka', 'naskah', 'yang', 'sudah', 'tua', 'itu', 'kata', 'darweni', 'karena', 'rawat', 'belum', 'standar', 'sejauh', 'ini', 'pihaknya', 'hanya', 'bisa', 'laku', 'rawat', 'harian', 'dengan', 'lebih', 'detail', 'cara', 'dengan', 'bubuh', 'kapur', 'barus', 'serta', 'rajin', 'bersih', 'dari', 'debu', 'sedikitnya', '4', 'kg', 'kapur', 'barus', 'habis', 'untuk', 'satu', 'ruang', 'itu', 'dan', 'wajib', 'selalu', 'ganti', 'selama', 'beberapa', 'hari', 'akhir', 'di', 'sini', 'udara', 'terlalu', 'lembab', 'sehingga', 'jamur', 'dan', 'kutu', 'mudah', 'rusak', 'buku', 'karenanya', 'dalam', 'perawatannya', 'kami', 'guna', 'sirio', 'black', 'biar', 'ph', 'dia', 'netral', 'kalau', 'awetnya', 'kami', 'hanya', 'guna', 'kapur', 'barus', 'saja', 'seharusnya', 'pasang', 'ac', 'agar', 'suhu', 'bisa', 'jaga', 'idealnya', 'suhu', 'tetap', 'jaga', 'pada', '18', 'derajat', 'celsius', 'selama', '24', 'jam', 'kata', 'darweni', 'sekadar', 'informasi', 'kawasan', 'keraton', 'pura', 'mangkunegaran', 'jadi', 'salah', 'satu', 'benda', 'cagar', 'budaya', 'sejak', '2013', 'silam', 'sesuai', 'dengan', 'putus', 'wali', 'kota', 'surakarta', 'no', '646', '1', '2', '1', '2013', 'tentang', 'tetap', 'bangun', 'dan', 'kawasan', 'kuno', 'sejarah', 'di', 'surakarta', 'karena', 'itu', 'dia', 'harap', 'ada', 'hati', 'lebih', 'dari', 'perintah', 'terutama', 'dalam', 'jaga', 'utuh', 'manuskrip', 'manuskrip', 'penting', 'tersebut', 'budaya', 'baca', 'arsip', 'dan', 'buku', 'kuno', 'jadi', 'penting', 'untuk', 'buka', 'mata', 'generasi', 'baru', 'bahwa', 'sejarah', 'dahulu', 'yang', 'bawa', 'kita', 'jadi', 'seperti', 'saat', 'ini', 'oleh', 'sebab', 'itu', 'kami', 'ingin', 'perintah', 'lebih', 'hati', 'lagi', 'terhadap', 'manuskrip', 'kuno', 'yang', 'ada', 'di', 'sini', 'ujar', 'darweni', 'mg4', 'ves', 'bun']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize\n",
    "from stanza.models.common.doc import Document\n",
    "from stanza.pipeline.core import Pipeline\n",
    "nlp_stanza: Pipeline = stanza.Pipeline(\n",
    "  'id',\n",
    "  processors = 'tokenize,mwt,lemma',\n",
    "  tokenize_pretokenized = True,\n",
    ")\n",
    "\n",
    "def lemmatize_tokens_id(tokens: List[str], nlp_stanza: Pipeline) -> List[str]:\n",
    "  doc: Document = nlp_stanza([tokens])\n",
    "\n",
    "  tokens_lemmatized: List[str] = [\n",
    "    word.lemma for sent in doc.sentences for word in sent.words\n",
    "  ]\n",
    "  tokens_lemmatized_without_empty_string: List[str] = [\n",
    "    token\n",
    "    for token in tokens_lemmatized\n",
    "    if (token != None) and (token != '')\n",
    "  ]\n",
    "\n",
    "  return tokens_lemmatized_without_empty_string\n",
    "\n",
    "example_tokens_lemmatized: List[str] = lemmatize_tokens_id(\n",
    "  tokens = example_tokens,\n",
    "  nlp_stanza = nlp_stanza,\n",
    ")\n",
    "print(f\"example of tokens before lemmatization: {example_tokens}\")\n",
    "print(f\"example of tokens after lemmatization:  {example_tokens_lemmatized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1v0Jw7FhC_2G"
   },
   "source": [
    "### 1.3: Stemming\n",
    "\n",
    "Stemming, mirip dengan lematisasi, merupakan aktivitas transformasi kata dari bentuk varian atau *inflected* menjadi bentuk dasar. Bedanya, stemming menggunakan metode algorithmic, yaitu sekumpulan rules untuk memotong karakter (imbuhan) dalam token hingga mencapai bentuk dasar yang disebut stem.\n",
    "\n",
    "Contoh Bahasa Indonesia: \"memakan\", \"dimakan\", \"termakan\" -> \"makan\"\n",
    "\n",
    "Contoh Bahasa Inggris: \"wait\", \"waiting\", \"waits\" -> \"wait\"\n",
    "\n",
    "Dalam contoh ini, stemming dilakukan menggunakan tools dari [PySastrawi](https://github.com/har07/PySastrawi)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6OfloeHwvhTU",
    "ExecuteTime": {
     "end_time": "2023-09-12T14:22:57.326452200Z",
     "start_time": "2023-09-12T14:22:57.251824600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example of tokens before stemming: ['03', 'desember', '2018', '11', '55', '59', 'wib', 'editor', 'perdana', 'rawat', 'naskah', 'kuno', 'usia', 'ratus', 'tahun', 'koleksi', 'rekso', 'pustoko', 'perlu', 'biaya', 'tak', 'sedikit', 'biaya', 'dirinya', 'tak', 'sangkal', 'bahwa', 'rawat', 'untuk', 'buku', 'buku', 'usia', 'sekitar', '200', 'tahun', 'itu', 'bilang', 'kurang', 'besar', 'biaya', 'rawat', 'masih', 'jadi', 'alasan', 'utama', 'sebagian', 'besar', 'koleksi', 'belum', 'tersentuh', 'rawat', 'layak', 'naskah', 'dan', 'manuskrip', 'kuno', 'di', 'museum', 'museum', 'atau', 'perpustakaan', 'besar', 'lainnya', 'salah', 'satu', 'rawat', 'yang', 'masih', 'belum', 'baik', 'adalah', 'fumigasi', 'hingga', 'saat', 'ini', 'rawat', 'untuk', 'bunuh', 'biota', 'yang', 'rusak', 'arsip', 'hanya', 'bisa', 'laku', 'setahun', 'sekali', 'untuk', 'satu', 'ruang', 'idealnya', 'fumigasi', 'laku', 'dua', 'kali', 'dalam', 'setahun', 'selain', 'fumigasi', 'untuk', 'jaga', 'tahan', 'kertas', 'agar', 'tidak', 'dimakan', 'ngengat', 'bisa', 'laku', 'dengan', 'laku', 'enkapsulasi', 'dan', 'alih', 'media', 'biaya', 'enkapsulasi', 'sendiri', 'cukup', 'besar', 'satu', 'lembar', 'enkapsulasi', 'dihargai', 'sebesar', 'rp', '20', 'ribu', 'dalam', 'sebul', 'ekapsulasi', 'bisa', 'laku', 'pada', '1', '000', 'lembar', 'manuskrip', 'sehingga', 'biayanya', 'sekitar', '20', 'juta', 'karena', 'bahan', 'yang', 'cukup', 'mahal', 'serta', 'harus', 'ditangani', 'profesional', 'maka', 'prosesnya', 'cukup', 'makan', 'waktu', 'masalahnya', 'biaya', 'masih', 'laku', 'secara', 'mandiri', 'belum', 'lagi', 'minimnya', 'tenaga', 'ahli', 'di', 'sini', 'dalam', 'rawat', 'ujar', 'darweni', 'petugas', 'alih', 'aksara', 'rekso', 'pustoko', 'biaya', 'untuk', 'rawat', 'sendiri', 'andal', 'bantu', 'yayasan', 'suryosumirat', 'dan', 'bantu', 'dari', 'pihak', 'luar', 'maupun', 'masyarakat', 'pihak', 'keraton', 'belum', 'bisa', 'beri', 'dana', 'yang', 'cukup', 'untuk', 'bisa', 'rawat', 'arsip', 'kuno', 'dengan', 'baik', 'karena', 'kendala', 'dana', 'itu', 'proses', 'alih', 'media', 'dengan', 'digitalisasi', 'juga', 'belum', 'selesai', 'padahal', 'ini', 'sangat', 'bantu', 'agar', 'isi', 'teks', 'dapat', 'teliti', 'tanpa', 'buka', 'naskah', 'yang', 'sudah', 'tua', 'itu', 'kata', 'darweni', 'karena', 'rawat', 'belum', 'standar', 'sejauh', 'ini', 'pihaknya', 'hanya', 'bisa', 'laku', 'rawat', 'harian', 'dengan', 'lebih', 'detail', 'cara', 'dengan', 'bubuh', 'kapur', 'barus', 'serta', 'rajin', 'bersih', 'dari', 'debu', 'sedikitnya', '4', 'kg', 'kapur', 'barus', 'habis', 'untuk', 'satu', 'ruang', 'itu', 'dan', 'wajib', 'selalu', 'ganti', 'selama', 'beberapa', 'hari', 'akhir', 'di', 'sini', 'udara', 'terlalu', 'lembab', 'sehingga', 'jamur', 'dan', 'kutu', 'mudah', 'rusak', 'buku', 'karenanya', 'dalam', 'perawatannya', 'kami', 'guna', 'sirio', 'black', 'biar', 'ph', 'dia', 'netral', 'kalau', 'awetnya', 'kami', 'hanya', 'guna', 'kapur', 'barus', 'saja', 'seharusnya', 'pasang', 'ac', 'agar', 'suhu', 'bisa', 'jaga', 'idealnya', 'suhu', 'tetap', 'jaga', 'pada', '18', 'derajat', 'celsius', 'selama', '24', 'jam', 'kata', 'darweni', 'sekadar', 'informasi', 'kawasan', 'keraton', 'pura', 'mangkunegaran', 'jadi', 'salah', 'satu', 'benda', 'cagar', 'budaya', 'sejak', '2013', 'silam', 'sesuai', 'dengan', 'putus', 'wali', 'kota', 'surakarta', 'no', '646', '1', '2', '1', '2013', 'tentang', 'tetap', 'bangun', 'dan', 'kawasan', 'kuno', 'sejarah', 'di', 'surakarta', 'karena', 'itu', 'dia', 'harap', 'ada', 'hati', 'lebih', 'dari', 'perintah', 'terutama', 'dalam', 'jaga', 'utuh', 'manuskrip', 'manuskrip', 'penting', 'tersebut', 'budaya', 'baca', 'arsip', 'dan', 'buku', 'kuno', 'jadi', 'penting', 'untuk', 'buka', 'mata', 'generasi', 'baru', 'bahwa', 'sejarah', 'dahulu', 'yang', 'bawa', 'kita', 'jadi', 'seperti', 'saat', 'ini', 'oleh', 'sebab', 'itu', 'kami', 'ingin', 'perintah', 'lebih', 'hati', 'lagi', 'terhadap', 'manuskrip', 'kuno', 'yang', 'ada', 'di', 'sini', 'ujar', 'darweni', 'mg4', 'ves', 'bun']\n",
      "example of tokens after stemming:  ['03', 'desember', '2018', '11', '55', '59', 'wib', 'editor', 'perdana', 'rawat', 'naskah', 'kuno', 'usia', 'ratus', 'tahun', 'koleksi', 'rekso', 'pustoko', 'perlu', 'biaya', 'tak', 'sedikit', 'biaya', 'diri', 'tak', 'sangkal', 'bahwa', 'rawat', 'untuk', 'buku', 'buku', 'usia', 'sekitar', '200', 'tahun', 'itu', 'bilang', 'kurang', 'besar', 'biaya', 'rawat', 'masih', 'jadi', 'alas', 'utama', 'bagi', 'besar', 'koleksi', 'belum', 'sentuh', 'rawat', 'layak', 'naskah', 'dan', 'manuskrip', 'kuno', 'di', 'museum', 'museum', 'atau', 'pustaka', 'besar', 'lain', 'salah', 'satu', 'rawat', 'yang', 'masih', 'belum', 'baik', 'adalah', 'fumigasi', 'hingga', 'saat', 'ini', 'rawat', 'untuk', 'bunuh', 'biota', 'yang', 'rusak', 'arsip', 'hanya', 'bisa', 'laku', 'tahun', 'sekali', 'untuk', 'satu', 'ruang', 'ideal', 'fumigasi', 'laku', 'dua', 'kali', 'dalam', 'tahun', 'selain', 'fumigasi', 'untuk', 'jaga', 'tahan', 'kertas', 'agar', 'tidak', 'makan', 'ngengat', 'bisa', 'laku', 'dengan', 'laku', 'enkapsulasi', 'dan', 'alih', 'media', 'biaya', 'enkapsulasi', 'sendiri', 'cukup', 'besar', 'satu', 'lembar', 'enkapsulasi', 'harga', 'besar', 'rp', '20', 'ribu', 'dalam', 'sebul', 'ekapsulasi', 'bisa', 'laku', 'pada', '1', '000', 'lembar', 'manuskrip', 'sehingga', 'biaya', 'sekitar', '20', 'juta', 'karena', 'bahan', 'yang', 'cukup', 'mahal', 'serta', 'harus', 'tangan', 'profesional', 'maka', 'proses', 'cukup', 'makan', 'waktu', 'masalah', 'biaya', 'masih', 'laku', 'cara', 'mandiri', 'belum', 'lagi', 'minim', 'tenaga', 'ahli', 'di', 'sini', 'dalam', 'rawat', 'ujar', 'darweni', 'tugas', 'alih', 'aksara', 'rekso', 'pustoko', 'biaya', 'untuk', 'rawat', 'sendiri', 'andal', 'bantu', 'yayasan', 'suryosumirat', 'dan', 'bantu', 'dari', 'pihak', 'luar', 'maupun', 'masyarakat', 'pihak', 'keraton', 'belum', 'bisa', 'beri', 'dana', 'yang', 'cukup', 'untuk', 'bisa', 'rawat', 'arsip', 'kuno', 'dengan', 'baik', 'karena', 'kendala', 'dana', 'itu', 'proses', 'alih', 'media', 'dengan', 'digitalisasi', 'juga', 'belum', 'selesai', 'padahal', 'ini', 'sangat', 'bantu', 'agar', 'isi', 'teks', 'dapat', 'teliti', 'tanpa', 'buka', 'naskah', 'yang', 'sudah', 'tua', 'itu', 'kata', 'darweni', 'karena', 'rawat', 'belum', 'standar', 'jauh', 'ini', 'pihak', 'hanya', 'bisa', 'laku', 'rawat', 'hari', 'dengan', 'lebih', 'detail', 'cara', 'dengan', 'bubuh', 'kapur', 'barus', 'serta', 'rajin', 'bersih', 'dari', 'debu', 'sedikit', '4', 'kg', 'kapur', 'barus', 'habis', 'untuk', 'satu', 'ruang', 'itu', 'dan', 'wajib', 'selalu', 'ganti', 'lama', 'beberapa', 'hari', 'akhir', 'di', 'sini', 'udara', 'terlalu', 'lembab', 'sehingga', 'jamur', 'dan', 'kutu', 'mudah', 'rusak', 'buku', 'karena', 'dalam', 'awat', 'kami', 'guna', 'sirio', 'black', 'biar', 'ph', 'dia', 'netral', 'kalau', 'awet', 'kami', 'hanya', 'guna', 'kapur', 'barus', 'saja', 'harus', 'pasang', 'ac', 'agar', 'suhu', 'bisa', 'jaga', 'ideal', 'suhu', 'tetap', 'jaga', 'pada', '18', 'derajat', 'celsius', 'lama', '24', 'jam', 'kata', 'darweni', 'sekadar', 'informasi', 'kawasan', 'keraton', 'pura', 'mangkunegaran', 'jadi', 'salah', 'satu', 'benda', 'cagar', 'budaya', 'sejak', '2013', 'silam', 'sesuai', 'dengan', 'putus', 'wali', 'kota', 'surakarta', 'no', '646', '1', '2', '1', '2013', 'tentang', 'tetap', 'bangun', 'dan', 'kawasan', 'kuno', 'sejarah', 'di', 'surakarta', 'karena', 'itu', 'dia', 'harap', 'ada', 'hati', 'lebih', 'dari', 'perintah', 'utama', 'dalam', 'jaga', 'utuh', 'manuskrip', 'manuskrip', 'penting', 'sebut', 'budaya', 'baca', 'arsip', 'dan', 'buku', 'kuno', 'jadi', 'penting', 'untuk', 'buka', 'mata', 'generasi', 'baru', 'bahwa', 'sejarah', 'dahulu', 'yang', 'bawa', 'kita', 'jadi', 'seperti', 'saat', 'ini', 'oleh', 'sebab', 'itu', 'kami', 'ingin', 'perintah', 'lebih', 'hati', 'lagi', 'hadap', 'manuskrip', 'kuno', 'yang', 'ada', 'di', 'sini', 'ujar', 'darweni', 'mg4', 'ves', 'bun']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.Stemmer.CachedStemmer import CachedStemmer\n",
    "factory: StemmerFactory = StemmerFactory()\n",
    "stemmer: CachedStemmer = factory.create_stemmer()\n",
    "\n",
    "def stem_tokens_id(tokens: List[str], stemmer: CachedStemmer) -> List[str]:\n",
    "  stemmed_tokens: List[str] = [\n",
    "      stemmer.stem(token) if token else ''\n",
    "      for token in tokens\n",
    "  ]\n",
    "  stemmed_tokens_without_empty_string: List[str] = [\n",
    "      token\n",
    "      for token in stemmed_tokens\n",
    "      if not ((token == '') or (token == None))\n",
    "  ]\n",
    "  return stemmed_tokens_without_empty_string\n",
    "\n",
    "example_tokens_after_stemming: List[str] = stem_tokens_id(\n",
    "  tokens = example_tokens_lemmatized,\n",
    "  stemmer = stemmer,\n",
    ")\n",
    "print(f\"example of tokens before stemming: {example_tokens_lemmatized}\")\n",
    "print(f\"example of tokens after stemming:  {example_tokens_after_stemming}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKECKWA1Ddug"
   },
   "source": [
    "### 1.4: Stop Words Removal\n",
    "\n",
    "Stop word merupakan kata-kata yang pada umumnya memiliki frekuensi yang sangat tinggi dalam teks namun tidak memberikan informasi yang signifikan.\n",
    "\n",
    "Contoh stop words bahasa Indonesia: \"yang\", \"di\", \"pada\"\n",
    "\n",
    "Cotoh stop words bahasa Inggris: \"of\", \"in\", \"on\"\n",
    "\n",
    "Dalam contoh berikut, kita bisa mendapatkan kumpulan stop words dari tools PySastrawi dan mengunakannya untuk membersihkan token-token yang akan diproses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xEi8sFYLJZc2",
    "ExecuteTime": {
     "end_time": "2023-09-12T14:22:59.567720100Z",
     "start_time": "2023-09-12T14:22:59.539822100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop words from PySastrawi: {'dijelaskan', 'enggak', 'hingga', 'menjelaskan', 'berlalu', 'diperbuat', 'tiba', 'keadaan', 'yakin', 'diperlukannya', 'tegas', 'p', 'dahulu', 'ungkapnya', 'tepat', 'ditunjukkannya', 'lah', 'beginilah', 'dulu', 'ini', 'salam', 'masalahnya', 'bagian', 'jauh', 'dilalui', 'yang', 'ternyata', 'tentunya', 'sinilah', 'tersebut', 'terdiri', 'apatah', 'meski', 'per', 'bapak', 'sedangkan', 'kenapa', 'didapat', 'belakangan', 'dirinya', 'selamanya', 'inginkah', 'mengetahui', 'tandasnya', 'lainnya', 'dikerjakan', 'antar', 'bagainamakah', 'depan', 'keterlaluan', 'berawal', 'jelaslah', 'amatlah', 'tampak', 'bersama-sama', 'sebab', 'sepantasnya', 'pihak', 'itu', 'mungkin', 'ketika', 'sebagaimana', 'sendiri', 'semampunya', 'antara', 'sekiranya', 'atau', 'sementara', 'setidak-tidaknya', 'beginikah', 'kapanpun', 'lama', 'merekalah', 'selama-lamanya', 'bung', 'semuanya', 'terhadap', 'dikatakannya', 'memperlihatkan', 'sebaik-baiknya', 'bagaikan', 'semata-mata', 'tempat', 'benar', 'setibanya', 'diberikan', 'kebetulan', 'saya', 'katakanlah', 'ataukah', 'dipertanyakan', 'berikutnya', 'toh', 'menunjuki', 'sambil', 'q', 'terhadapnya', 'atas', 'tanpa', 'rupanya', 'setiba', 'rupa', 'f', 'bermacam', 'kapankah', 'sela', 'sendirinya', 'masuk', 'siapa', 'ungkap', 'bagaimanakah', 'tahu', 'lanjutnya', 'tadi', 'saatnya', 'dengan', 'enak', 'seharusnya', 'dimaksudkannya', 'masih', 'sudahkah', 'inikah', 'memperkirakan', 'sangatlah', 'bolehlah', 'sesudahnya', 'awalnya', 'janganlah', 'kira-kira', 'mulailah', 'mengibaratkannya', 'dia', 'harusnya', 'oleh', 'bermaksud', 'terkira', 'kemungkinan', 'memisalkan', 'kala', 'membuat', 'berturut', 'sekadar', 'terdapat', 'biasanya', 'misalkan', 'pihaknya', 'punya', 'ditunjukkan', 'jumlahnya', 'sebisanya', 'dijelaskannya', 'jangan', 'masihkah', 'sajalah', 'mendapatkan', 'manalagi', 'makanya', 'sekecil', 'kapan', 'sedikit', 'segalanya', 'se', 'kecil', 'persoalan', 'cukupkah', 'hampir', 'mendatangi', 'setempat', 'apakah', 'sudah', 'mendatang', 'selama', 'benarkah', 'ingat-ingat', 'guna', 'selanjutnya', 'terjadi', 'ujarnya', 'mohon', 'hadap', 'semisalnya', 'apalagi', 'agaknya', 'sudahlah', 'bagi', 'masing', 'mempersoalkan', 'ditujukan', 'sesampai', 'sekalian', 'pernah', 'pertama', 'bertanya-tanya', 'setinggi', 'tidak', 'mirip', 'aku', 'melihat', 'ibarat', 'tanya', 'berapalah', 'dekat', 'itukah', 'menunjuknya', 'mendatangkan', 'dapat', 'untuk', 'teringat-ingat', 'melakukan', 'asal', 'tapi', 'usah', 'seluruhnya', 'diperbuatnya', 'macam', 'di', 'harus', 'tuju', 'seringnya', 'menuju', 'yaitu', 'seolah', 'seketika', 'wah', 'l', 'begitulah', 'sangkut', 'semaunya', 'kira', 'paling', 'berupa', 'hendak', 'beberapa', 'begini', 'mereka', 'diberi', 'balik', 'ia', 'sekurangnya', 'selalu', 'seterusnya', 'k', 'disampaikan', 'dilakukan', 'kemudian', 'mampukah', 'semula', 'disebutkan', 'nah', 'melalui', 'kelihatannya', 'menyangkut', 'tersebutlah', 'siap', 'dipunyai', 'pukul', 'saling', 'sebaik', 'besar', 'bawah', 'ingat', 'jawabnya', 'w', 'bila', 'banyak', 'menggunakan', 'begitu', 'berbagai', 'tertentu', 'memastikan', 'kerja', 'pertanyakan', 'dibuat', 'keluar', 'siapapun', 'menginginkan', 'disebutkannya', 'semua', 'sepanjang', 'ataupun', 'mengingatkan', 'masalah', 'diibaratkannya', 'bulan', 'meyakini', 'katanya', 'perlu', 'akankah', 'kiranya', 'mau', 'sekaligus', 'mengapa', 'bagaimanapun', 'cara', 'inginkan', 'berapapun', 'keinginan', 'minta', 'sesekali', 'tak', 'sesegera', 'terutama', 'mengucapkan', 'mempertanyakan', 'menghendaki', 'terbanyak', 'karenanya', 'sebutnya', 'diperkirakan', 'baik', 'd', 'halo', 'jadilah', 'kalaulah', 'akulah', 'sebuah', 'ucap', 'tanyanya', 'pastilah', 'ditegaskan', 'jawaban', 'mata', 'diketahui', 'tiga', 'amat', 'sepantasnyalah', 'memerlukan', 'hallo', 'tidakkah', 'diantara', 'kelamaan', 'malah', 'seberapa', 'bertanya', 'padahal', 'dimaksudnya', 'dari', 'sebetulnya', 'sini', 'wahai', 'kalian', 'pak', 'terlalu', 'pentingnya', 'diperlihatkan', 'kena', 'menjawab', 'terlebih', 'menanti', 'kata', 'juga', 'sekitarnya', 'kami', 'tegasnya', 'diinginkan', 'kitalah', 'dimulailah', 'awal', 'mempersiapkan', 'lanjut', 'tiap', 'kalau', 'akhir', 'berkeinginan', 'soal', 'khusus', 'terlihat', 'turut', 'mungkinkah', 'kepada', 'diperlukan', 'a', 'cukup', 'antaranya', 'c', 'pantas', 'sama-sama', 'mempunyai', 'memungkinkan', 'berlainan', 'maka', 'berakhirlah', 'selaku', 'pada', 'semisal', 'tadinya', 'misal', 'nantinya', 'supaya', 'bekerja', 'berikan', 'bermula', 'hendaklah', 'kemungkinannya', 'memulai', 'semampu', 'semasih', 'meskipun', 'diingat', 'lagi', 'z', 'akan', 'diri', 'kesampaian', 'mulanya', 'sekitar', 'hendaknya', 'kembali', 'saat', 'berakhirnya', 'memberi', 'seperti', 'dini', 'empat', 'semacam', 'ibaratnya', 'sebenarnya', 'biasa', 'buat', 'sama', 'saja', 'kadar', 'meminta', 'orang', 'dimaksudkan', 'dua', 'i', 'secara', 'diungkapkan', 'dimungkinkan', 'apaan', 'anda', 'bersiap-siap', 'lebih', 'sendirian', 'dimisalkan', 'meyakinkan', 'diucapkan', 'bahwa', 'kedua', 'suatu', 'langsung', 'memintakan', 'dialah', 'n', 'baiklah', 'lima', 'sebaiknya', 'nyaris', 'makin', 'mempergunakan', 'dimulai', 'kita', 'bagai', 'berarti', 'asalkan', 'terus', 'tidaklah', 't', 'tambahnya', 'semasa', 'apa', 'jelaskan', 'sampaikan', 'berkehendak', 'wong', 'terakhir', 'keseluruhannya', 'menambahkan', 'caranya', 'lewat', 'dituturkan', 'hanya', 'keseluruhan', 'r', 'luar', 'dibuatnya', 'ditandaskan', 'didatangkan', 'dilihat', 'segera', 'sekaranglah', 'belumlah', 'setiap', 'berujar', 'dituturkannya', 'mengerjakan', 'umumnya', 'v', 'sekurang-kurangnya', 'u', 'ibaratkan', 'sekali', 'berapa', 'bukanlah', 'menyeluruh', 'berapakah', 'tentu', 'tunjuk', 'sebelum', 'ditanya', 'walaupun', 'sekali-kali', 'ditambahkan', 'sejenak', 'sepihak', 'disebut', 'sebagai', 'secukupnya', 'bagaimana', 'sampai', 'tertuju', 'sewaktu', 'begitukah', 'terasa', 'masing-masing', 'tutur', 'maupun', 'kepadanya', 'siapakah', 'diakhiri', 'berkali-kali', 'yakni', 'ibu', 'agak', 'diantaranya', 'ditunjuki', 'hanyalah', 'hai', 'jangankan', 'malahan', 'sebelumnya', 'memihak', 'misalnya', 'dimintai', 'seenaknya', 'boleh', 'mengatakan', 'sebutlah', 'ada', 'dalam', 'menanyakan', 'sempat', 'merupakan', 'jelasnya', 'semakin', 'sebaliknya', 'jawab', 'sekadarnya', 'begitupun', 'j', 'maksud', 'bermacam-macam', 'sebegitu', 'waduh', 'tetapi', 'berturut-turut', 'bukannya', 'disini', 'ditanyakan', 'tuturnya', 'namun', 'olehnya', 'adalah', 'menantikan', 'segala', 'entah', 'beginian', 'menaiki', 'sepertinya', 'baru', 'belum', 'menegaskan', 'tiba-tiba', 'dikira', 'jika', 'ikut', 'adanya', 'bertutur', 'menuturkan', 's', 'mengucapkannya', 'memang', 'perlunya', 'dimulainya', 'e', 'keduanya', 'x', 'percuma', 'karena', 'tandas', 'bolehkah', 'b', 'sering', 'hari', 'para', 'manakala', 'sebanyak', 'masa', 'diakhirinya', 'sesuatunya', 'setidaknya', 'arti', 'sana', 'setengah', 'artinya', 'penting', 'satu', 'ujar', 'terjadinya', 'diminta', 'tersampaikan', 'akhiri', 'usai', 'berkata', 'berlebihan', 'bakalan', 'nya', 'berjumlah', 'demikian', 'demikianlah', 'rasanya', 'betul', 'menyiapkan', 'sedemikian', 'hello', 'ditunjuknya', 'bersiap', 'perlukah', 'dipergunakan', 'ditunjuk', 'setelah', 'tambah', 'telah', 'berlangsung', 'kamu', 'sebesar', 'mampu', 'apabila', 'dan', 'padanya', 'tentang', 'enggaknya', 'disinilah', 'mulai', 'kok', 'cuma', 'beri', 'nyatanya', 'kelima', 'kamulah', 'menandaskan', 'menyebutkan', 'lihat', 'laku', 'dipastikan', 'dikarenakan', 'sebut', 'kamilah', 'sampai-sampai', 'sekarang', 'jumlah', 'bersama', 'hal', 'nyata', 'rasa', 'o', 'soalnya', 'tetap', 'melihatnya', 'sebagian', 'seorang', 'h', 'demi', 'betulkah', 'jadi', 'benarlah', 'tanyakan', 'seluruh', 'dong', 'bukankah', 'memperbuat', 'panjang', 'ingin', 'menanti-nanti', 'mengungkapkan', 'cukuplah', 'diucapkannya', 'mengira', 'mana', 'pula', 'kurang', 'bahwasanya', 'gunakan', 'mendapat', 'ditanyai', 'semata', 'sejak', 'mengakhiri', 'khususnya', 'sebegini', 'mengingat', 'melainkan', 'dijawab', 'kali', 'tentulah', 'itulah', 'ialah', 'dimaksud', 'belakang', 'mengibaratkan', 'berikut', 'pasti', 'lain', 'dipersoalkan', 'katakan', 'menunjukkan', 'helo', 'berkenaan', 'jelas', 'g', 'haruslah', 'm', 'nanti', 'selain', 'memberikan', 'menunjuk', 'sebabnya', 'diberikannya', 'bahwasannya', 'daripada', 'diketahuinya', 'kalaupun', 'jadinya', 'sejumlah', 'serta', 'sejauh', 'sesudah', 'walau', 'inilah', 'lalu', 'entahlah', 'kini', 'berada', 'akhirnya', 'menanya', 'terdahulu', 'lamanya', 'bisakah', 'mengatakannya', 'digunakan', 'sedikitnya', 'kan', 'seusai', 'naik', 'seolah-olah', 'tampaknya', 'waktunya', 'justru', 'menyatakan', 'mengenai', 'sedang', 'merasa', 'mula', 'berakhir', 'adapun', 'andalah', 'pun', 'terjadilah', 'menurut', 'jikalau', 'sebagainya', 'menyampaikan', 'bisa', 'diibaratkan', 'agar', 'sayalah', 'sesuatu', 'termasuk', 'sehingga', 'teringat', 'bukan', 'bakal', 'ya', 'diingatkan', 'sesaat', 'ucapnya', 'menjadi', 'y', 'kelihatan', 'serupa', 'seseorang', 'kinilah', 'datang', 'berdatangan', 'bahkan', 'ke', 'sesama', 'seperlunya', 'sangat', 'bilakah', 'pertama-tama', 'menanyai', 'pertanyaan', 'sekalipun', 'lagian', 'kasus', 'dikatakan', 'seingat'}\n",
      "\n",
      "example of tokens with stop words (before):   ['03', 'desember', '2018', '11', '55', '59', 'wib', 'editor', 'perdana', 'rawat', 'naskah', 'kuno', 'usia', 'ratus', 'tahun', 'koleksi', 'rekso', 'pustoko', 'perlu', 'biaya', 'tak', 'sedikit', 'biaya', 'diri', 'tak', 'sangkal', 'bahwa', 'rawat', 'untuk', 'buku', 'buku', 'usia', 'sekitar', '200', 'tahun', 'itu', 'bilang', 'kurang', 'besar', 'biaya', 'rawat', 'masih', 'jadi', 'alas', 'utama', 'bagi', 'besar', 'koleksi', 'belum', 'sentuh', 'rawat', 'layak', 'naskah', 'dan', 'manuskrip', 'kuno', 'di', 'museum', 'museum', 'atau', 'pustaka', 'besar', 'lain', 'salah', 'satu', 'rawat', 'yang', 'masih', 'belum', 'baik', 'adalah', 'fumigasi', 'hingga', 'saat', 'ini', 'rawat', 'untuk', 'bunuh', 'biota', 'yang', 'rusak', 'arsip', 'hanya', 'bisa', 'laku', 'tahun', 'sekali', 'untuk', 'satu', 'ruang', 'ideal', 'fumigasi', 'laku', 'dua', 'kali', 'dalam', 'tahun', 'selain', 'fumigasi', 'untuk', 'jaga', 'tahan', 'kertas', 'agar', 'tidak', 'makan', 'ngengat', 'bisa', 'laku', 'dengan', 'laku', 'enkapsulasi', 'dan', 'alih', 'media', 'biaya', 'enkapsulasi', 'sendiri', 'cukup', 'besar', 'satu', 'lembar', 'enkapsulasi', 'harga', 'besar', 'rp', '20', 'ribu', 'dalam', 'sebul', 'ekapsulasi', 'bisa', 'laku', 'pada', '1', '000', 'lembar', 'manuskrip', 'sehingga', 'biaya', 'sekitar', '20', 'juta', 'karena', 'bahan', 'yang', 'cukup', 'mahal', 'serta', 'harus', 'tangan', 'profesional', 'maka', 'proses', 'cukup', 'makan', 'waktu', 'masalah', 'biaya', 'masih', 'laku', 'cara', 'mandiri', 'belum', 'lagi', 'minim', 'tenaga', 'ahli', 'di', 'sini', 'dalam', 'rawat', 'ujar', 'darweni', 'tugas', 'alih', 'aksara', 'rekso', 'pustoko', 'biaya', 'untuk', 'rawat', 'sendiri', 'andal', 'bantu', 'yayasan', 'suryosumirat', 'dan', 'bantu', 'dari', 'pihak', 'luar', 'maupun', 'masyarakat', 'pihak', 'keraton', 'belum', 'bisa', 'beri', 'dana', 'yang', 'cukup', 'untuk', 'bisa', 'rawat', 'arsip', 'kuno', 'dengan', 'baik', 'karena', 'kendala', 'dana', 'itu', 'proses', 'alih', 'media', 'dengan', 'digitalisasi', 'juga', 'belum', 'selesai', 'padahal', 'ini', 'sangat', 'bantu', 'agar', 'isi', 'teks', 'dapat', 'teliti', 'tanpa', 'buka', 'naskah', 'yang', 'sudah', 'tua', 'itu', 'kata', 'darweni', 'karena', 'rawat', 'belum', 'standar', 'jauh', 'ini', 'pihak', 'hanya', 'bisa', 'laku', 'rawat', 'hari', 'dengan', 'lebih', 'detail', 'cara', 'dengan', 'bubuh', 'kapur', 'barus', 'serta', 'rajin', 'bersih', 'dari', 'debu', 'sedikit', '4', 'kg', 'kapur', 'barus', 'habis', 'untuk', 'satu', 'ruang', 'itu', 'dan', 'wajib', 'selalu', 'ganti', 'lama', 'beberapa', 'hari', 'akhir', 'di', 'sini', 'udara', 'terlalu', 'lembab', 'sehingga', 'jamur', 'dan', 'kutu', 'mudah', 'rusak', 'buku', 'karena', 'dalam', 'awat', 'kami', 'guna', 'sirio', 'black', 'biar', 'ph', 'dia', 'netral', 'kalau', 'awet', 'kami', 'hanya', 'guna', 'kapur', 'barus', 'saja', 'harus', 'pasang', 'ac', 'agar', 'suhu', 'bisa', 'jaga', 'ideal', 'suhu', 'tetap', 'jaga', 'pada', '18', 'derajat', 'celsius', 'lama', '24', 'jam', 'kata', 'darweni', 'sekadar', 'informasi', 'kawasan', 'keraton', 'pura', 'mangkunegaran', 'jadi', 'salah', 'satu', 'benda', 'cagar', 'budaya', 'sejak', '2013', 'silam', 'sesuai', 'dengan', 'putus', 'wali', 'kota', 'surakarta', 'no', '646', '1', '2', '1', '2013', 'tentang', 'tetap', 'bangun', 'dan', 'kawasan', 'kuno', 'sejarah', 'di', 'surakarta', 'karena', 'itu', 'dia', 'harap', 'ada', 'hati', 'lebih', 'dari', 'perintah', 'utama', 'dalam', 'jaga', 'utuh', 'manuskrip', 'manuskrip', 'penting', 'sebut', 'budaya', 'baca', 'arsip', 'dan', 'buku', 'kuno', 'jadi', 'penting', 'untuk', 'buka', 'mata', 'generasi', 'baru', 'bahwa', 'sejarah', 'dahulu', 'yang', 'bawa', 'kita', 'jadi', 'seperti', 'saat', 'ini', 'oleh', 'sebab', 'itu', 'kami', 'ingin', 'perintah', 'lebih', 'hati', 'lagi', 'hadap', 'manuskrip', 'kuno', 'yang', 'ada', 'di', 'sini', 'ujar', 'darweni', 'mg4', 'ves', 'bun']\n",
      "example of tokens without stop words (after): ['03', 'desember', '2018', '11', '55', '59', 'wib', 'editor', 'perdana', 'rawat', 'naskah', 'kuno', 'usia', 'ratus', 'tahun', 'koleksi', 'rekso', 'pustoko', 'biaya', 'biaya', 'sangkal', 'rawat', 'buku', 'buku', 'usia', '200', 'tahun', 'bilang', 'biaya', 'rawat', 'alas', 'utama', 'koleksi', 'sentuh', 'rawat', 'layak', 'naskah', 'manuskrip', 'kuno', 'museum', 'museum', 'pustaka', 'salah', 'rawat', 'fumigasi', 'rawat', 'bunuh', 'biota', 'rusak', 'arsip', 'tahun', 'ruang', 'ideal', 'fumigasi', 'tahun', 'fumigasi', 'jaga', 'tahan', 'kertas', 'makan', 'ngengat', 'enkapsulasi', 'alih', 'media', 'biaya', 'enkapsulasi', 'lembar', 'enkapsulasi', 'harga', 'rp', '20', 'ribu', 'sebul', 'ekapsulasi', '1', '000', 'lembar', 'manuskrip', 'biaya', '20', 'juta', 'bahan', 'mahal', 'tangan', 'profesional', 'proses', 'makan', 'waktu', 'biaya', 'mandiri', 'minim', 'tenaga', 'ahli', 'rawat', 'darweni', 'tugas', 'alih', 'aksara', 'rekso', 'pustoko', 'biaya', 'rawat', 'andal', 'bantu', 'yayasan', 'suryosumirat', 'bantu', 'masyarakat', 'keraton', 'dana', 'rawat', 'arsip', 'kuno', 'kendala', 'dana', 'proses', 'alih', 'media', 'digitalisasi', 'selesai', 'bantu', 'isi', 'teks', 'teliti', 'buka', 'naskah', 'tua', 'darweni', 'rawat', 'standar', 'rawat', 'detail', 'bubuh', 'kapur', 'barus', 'rajin', 'bersih', 'debu', '4', 'kg', 'kapur', 'barus', 'habis', 'ruang', 'wajib', 'ganti', 'udara', 'lembab', 'jamur', 'kutu', 'mudah', 'rusak', 'buku', 'awat', 'sirio', 'black', 'biar', 'ph', 'netral', 'awet', 'kapur', 'barus', 'pasang', 'ac', 'suhu', 'jaga', 'ideal', 'suhu', 'jaga', '18', 'derajat', 'celsius', '24', 'jam', 'darweni', 'informasi', 'kawasan', 'keraton', 'pura', 'mangkunegaran', 'salah', 'benda', 'cagar', 'budaya', '2013', 'silam', 'sesuai', 'putus', 'wali', 'kota', 'surakarta', 'no', '646', '1', '2', '1', '2013', 'bangun', 'kawasan', 'kuno', 'sejarah', 'surakarta', 'harap', 'hati', 'perintah', 'utama', 'jaga', 'utuh', 'manuskrip', 'manuskrip', 'budaya', 'baca', 'arsip', 'buku', 'kuno', 'buka', 'generasi', 'sejarah', 'bawa', 'perintah', 'hati', 'manuskrip', 'kuno', 'darweni', 'mg4', 'ves', 'bun']\n"
     ]
    }
   ],
   "source": [
    "# Stop Words Removal\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "stop_factory = StopWordRemoverFactory()\n",
    "stop_words_list: List[str] = stop_factory.get_stop_words()\n",
    "stop_words_set: Set[str] = set(stop_words_list)\n",
    "\n",
    "def remove_stop_words_id(tokens: List[str], stop_words: Dict[str, Any]) -> List[str]:\n",
    "  tokens_without_stop_words: List[str] = [\n",
    "      token\n",
    "      for token in tokens\n",
    "      if token not in stop_words\n",
    "  ]\n",
    "  return tokens_without_stop_words\n",
    "\n",
    "example_tokens_without_stop_words: List[str] = remove_stop_words_id(\n",
    "  tokens = example_tokens_after_stemming,\n",
    "  stop_words = stop_words_set,\n",
    ")\n",
    "\n",
    "print(f\"stop words from PySastrawi: {stop_words_set}\\n\")\n",
    "print(f\"example of tokens with stop words (before):   {example_tokens_after_stemming}\")\n",
    "print(f\"example of tokens without stop words (after): {example_tokens_without_stop_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9YXU1bauNSd"
   },
   "source": [
    "### 1.5: Preprocess Data\n",
    "\n",
    "Function-function diatas (tokenisasi, lematisasi, *stemming*, *stop words removal*) dapat dirangkai untuk diterapkan terhadap seluruh data dalam dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2u_gTojJ-8xI",
    "ExecuteTime": {
     "end_time": "2023-09-12T14:23:40.240712500Z",
     "start_time": "2023-09-12T14:23:40.229586900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define your preprocessing pipeline as a function\n",
    "def preprocess_text_into_tokens_id(text: str,\n",
    "                                   tokenizer_pattern: str,\n",
    "                                   nlp_stanza,\n",
    "                                   stemmer: CachedStemmer,\n",
    "                                   stop_words: Dict[str, Any]) -> List[str]:\n",
    "  tokens: List[str] = tokenize_text_id(\n",
    "    text = text,\n",
    "    tokenizer_pattern = tokenizer_pattern,\n",
    "  )\n",
    "  tokens: List[str] = lemmatize_tokens_id(\n",
    "    tokens = tokens,\n",
    "    nlp_stanza = nlp_stanza,\n",
    "  )\n",
    "  tokens: List[str] = stem_tokens_id(\n",
    "    tokens = tokens,\n",
    "    stemmer = stemmer,\n",
    "  )\n",
    "  tokens: List[str] = remove_stop_words_id(\n",
    "    tokens = tokens,\n",
    "    stop_words = stop_words,\n",
    "  )\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K96uhv_9bXCw"
   },
   "source": [
    "Untuk menerapkan pipeline pemrosesan teks tersebut kepada seluruh artikel dalam dataset, kita dapat memanfaatkan built-in function map() dalam HuggingFace dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "tsBTmgTvATZu",
    "ExecuteTime": {
     "end_time": "2023-09-12T14:54:28.616664500Z",
     "start_time": "2023-09-12T14:42:39.062890300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/803 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "de272f13339b47c9b0d5c929a904c4de"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed dataset: Dataset({\n",
      "    features: ['tokens'],\n",
      "    num_rows: 803\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocess to all data\n",
    "dataset_preprocessed_id: Dataset = dataset_id.map(\n",
    "  lambda row: dict(\n",
    "    tokens = preprocess_text_into_tokens_id(\n",
    "      text = row['text'],\n",
    "      tokenizer_pattern = tokenizer_pattern,\n",
    "      nlp_stanza = nlp_stanza,\n",
    "      stemmer = stemmer,\n",
    "      stop_words = stop_words_set,\n",
    "    ),\n",
    "  ),\n",
    "  num_proc = 1,\n",
    "  remove_columns = ['text'],\n",
    ")\n",
    "print(f\"preprocessed dataset: {dataset_preprocessed_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfkg1yMfbIEX"
   },
   "source": [
    "Hasil akhir dari penerapan tersebut adalah data (tokens) yang siap digunakan untuk pemrosesan selanjutnya dalam aplikasi IR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Tq3lE7-8VejQ",
    "ExecuteTime": {
     "end_time": "2023-09-12T15:01:38.132287900Z",
     "start_time": "2023-09-12T15:01:38.114288700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample of original text (before): 03 Desember 2018, 11: 55: 59 WIB | editor : Perdana\n",
      "PERAWATAN naskah kuno berusia ratusan tahun koleksi Rekso Pustoko memerlukan biaya tak sedikit biaya. Dirinya tak menyangkal bahwa perawatan untuk buku-buku berusia sekitar 200 tahun itu terbilang kurang. Besarnya biaya perawatan masih menjadi alasan utama sebagian besar koleksi belum tersentuh perawatan layaknya naskah dan manuskrip kuno di museum-museum atau perpustakaan besar lainnya.\n",
      "Salah satu perawatan yang masih belum baik adalah fumigasi. Hingga saat ini, perawatan untuk membunuh biota yang merusak arsip hanya bisa dilakukan setahun sekali untuk satu ruang. Idealnya fumigasi dilakukan dua kali dalam setahun.\n",
      "Selain fumigasi, untuk menjaga ketahanan kertas agar tidak dimakan ngengat bisa dilakukan dengan melakukan enkapsulasi dan alih media. Biaya enkapsulasi sendiri cukup besar. Satu lembar enkapsulasi dihargai sebesar Rp 20 ribu. Dalam sebulan ekapsulasi bisa dilakukan pada 1.000 lembar manuskrip sehingga biayanya sekitar 20 juta. Karena bahan yang cukup mahal serta harus ditangani profesional maka prosesnya cukup memakan waktu.\n",
      "“Masalahnya pembiayaan masih dilakukan secara mandiri. Belum lagi minimnya tenaga ahli di sini dalam perawatan,” ujar Darweni, petugas alih aksara Rekso Pustoko .\n",
      "Biaya untuk perawatan sendiri mengandalkan bantuan Yayasan Suryosumirat dan bantuan dari pihak luar maupun masyarakat. Pihak keraton belum bisa memberikan dana yang cukup untuk bisa merawat arsip kuno dengan baik.\n",
      "“Karena kendala dana itu, proses alih media dengan digitalisasi juga belum selesai. Padahal ini sangat membantu agar isi teks dapat diteliti tanpa membuka naskah yang sudah tua itu,” kata Darweni.\n",
      "Karena perawatan belum standar, sejauh ini pihaknya hanya bisa melakukan perawatan harian dengan lebih detail. Caranya dengan membubuhkan kapur barus serta rajin membersihkan dari debu. Sedikitnya 4 kg kapur barus dihabiskan untuk satu ruangan itu dan wajib selalu diganti selama beberapa hari terakhir.\n",
      "“Di sini udaranya terlalu lembab, sehingga jamur dan kutu mudah merusak buku. Karenanya dalam perawatannya kami menggunakan sirio black biar PH nya netral. Kalau pengawetnya kami hanya menggunakan kapur barus saja. Seharusnya dipasang AC, agar suhunya bisa terjaga. Idealnya suhu tetap terjaga pada 18 derajat celsius selama 24 jam,” kata Darweni.\n",
      "Sekadar informasi, kawasan Keraton Pura Mangkunegaran menjadi salah satu Benda Cagar Budaya sejak 2013 silam. Sesuai dengan Keputusan Wali Kota Surakarta No. 646/1-2/1/2013 tentang Penetapan Bangunan dan Kawasan Kuno Bersejarah di Surakarta. Karena itu ia berharap ada perhatian lebih dari pemerintah terutama dalam menjaga keutuhan manuskrip-manuskrip penting tersebut.\n",
      "“Budaya membaca arsip dan buku kuno menjadi penting untuk membuka mata generasi baru, bahwa sejarah dahulu yang membawa kita menjadi seperti saat ini. oleh sebab itu kami ingin pemerintah lebih perhatian lagi terhadap manuskrip kuno yang ada di sini,” ujar Darweni. (mg4/ves/bun)\n",
      "------\n",
      "sample of preprocessed tokens (after): ['03', 'desember', '2018', '11', '55', '59', 'wib', 'editor', 'perdana', 'rawat', 'naskah', 'kuno', 'usia', 'ratus', 'tahun', 'koleksi', 'rekso', 'pustoko', 'biaya', 'biaya', 'sangkal', 'rawat', 'buku', 'buku', 'usia', '200', 'tahun', 'bilang', 'biaya', 'rawat', 'alas', 'utama', 'koleksi', 'sentuh', 'rawat', 'layak', 'naskah', 'manuskrip', 'kuno', 'museum', 'museum', 'pustaka', 'salah', 'rawat', 'fumigasi', 'rawat', 'bunuh', 'biota', 'rusak', 'arsip', 'tahun', 'ruang', 'ideal', 'fumigasi', 'tahun', 'fumigasi', 'jaga', 'tahan', 'kertas', 'makan', 'ngengat', 'enkapsulasi', 'alih', 'media', 'biaya', 'enkapsulasi', 'lembar', 'enkapsulasi', 'harga', 'rp', '20', 'ribu', 'sebul', 'ekapsulasi', '1', '000', 'lembar', 'manuskrip', 'biaya', '20', 'juta', 'bahan', 'mahal', 'tangan', 'profesional', 'proses', 'makan', 'waktu', 'biaya', 'mandiri', 'minim', 'tenaga', 'ahli', 'rawat', 'darweni', 'tugas', 'alih', 'aksara', 'rekso', 'pustoko', 'biaya', 'rawat', 'andal', 'bantu', 'yayasan', 'suryosumirat', 'bantu', 'masyarakat', 'keraton', 'dana', 'rawat', 'arsip', 'kuno', 'kendala', 'dana', 'proses', 'alih', 'media', 'digitalisasi', 'selesai', 'bantu', 'isi', 'teks', 'teliti', 'buka', 'naskah', 'tua', 'darweni', 'rawat', 'standar', 'rawat', 'detail', 'bubuh', 'kapur', 'barus', 'rajin', 'bersih', 'debu', '4', 'kg', 'kapur', 'barus', 'habis', 'ruang', 'wajib', 'ganti', 'udara', 'lembab', 'jamur', 'kutu', 'mudah', 'rusak', 'buku', 'awat', 'sirio', 'black', 'biar', 'ph', 'netral', 'awet', 'kapur', 'barus', 'pasang', 'ac', 'suhu', 'jaga', 'ideal', 'suhu', 'jaga', '18', 'derajat', 'celsius', '24', 'jam', 'darweni', 'informasi', 'kawasan', 'keraton', 'pura', 'mangkunegaran', 'salah', 'benda', 'cagar', 'budaya', '2013', 'silam', 'sesuai', 'putus', 'wali', 'kota', 'surakarta', 'no', '646', '1', '2', '1', '2013', 'bangun', 'kawasan', 'kuno', 'sejarah', 'surakarta', 'harap', 'hati', 'perintah', 'utama', 'jaga', 'utuh', 'manuskrip', 'manuskrip', 'budaya', 'baca', 'arsip', 'buku', 'kuno', 'buka', 'generasi', 'sejarah', 'bawa', 'perintah', 'hati', 'manuskrip', 'kuno', 'darweni', 'mg4', 'ves', 'bun']\n"
     ]
    }
   ],
   "source": [
    "print(f\"sample of original text (before): {dataset_id[0]['text']}\")\n",
    "print('------')\n",
    "print(f\"sample of preprocessed tokens (after): {dataset_preprocessed_id[0]['tokens']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "nW6ve4zEkBmE",
    "ExecuteTime": {
     "end_time": "2023-09-12T15:01:39.520136100Z",
     "start_time": "2023-09-12T15:01:39.511012200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagian 1: done in 42.18 minutes\n"
     ]
    }
   ],
   "source": [
    "end_time_id: int = time.time()\n",
    "duration_sec_id: int = end_time_id - start_time_id\n",
    "duration_min_id: float = round(duration_sec_id / 60, 2)\n",
    "print(f\"Bagian 1: done in {duration_min_id} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5zIqIub1HrI"
   },
   "source": [
    "## Bagian 2: Text Preprocessing Bahasa Inggris\n",
    "\n",
    "Dalam bagian ini, anda diminta untuk melakukan eksplorasi terhadap pemrosesan awal teks seperti pada bagian 1 namun diterapkan ke data dalam bahasa Inggris.\n",
    "\n",
    "Anda diperbolehkan menambah *code cell* baru sesuai kebutuhan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "5E9ej_fewnKS",
    "ExecuteTime": {
     "end_time": "2023-09-12T15:05:28.160894700Z",
     "start_time": "2023-09-12T15:05:28.143201400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagian 2: started at Tuesday, 12 September 2023, 22:05:28\n"
     ]
    }
   ],
   "source": [
    "start_time_en: int = time.time()\n",
    "time_struct = time.localtime(start_time_en)\n",
    "formatted_time = time.strftime(\"%A, %d %B %Y, %H:%M:%S\", time_struct)\n",
    "print(f\"Bagian 2: started at {formatted_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mDz8o4G06Gf"
   },
   "source": [
    "### Load Data\n",
    "\n",
    "Dalam tugas eksplorasi ini, anda akan menggunakan data corpus dari Common Crawl untuk bahasa Inggris dalam domain spesifik artikel-artikel berita. Code berikut diberikan untuk melakukan load data tersebut.\n",
    "\n",
    "Referensi:\n",
    "- https://huggingface.co/datasets/cc_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "KDyp8IPW2KoX",
    "ExecuteTime": {
     "end_time": "2023-09-12T15:12:44.916331700Z",
     "start_time": "2023-09-12T15:05:54.526408100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/4.32k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "979fb3412ce9456aac7ff977f9a5749f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading readme:   0%|          | 0.00/7.46k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "abfb9bc74aed40b2a9ac812f27d37f4a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset cc_news/plain_text to C:/Users/USER/.cache/huggingface/datasets/cc_news/plain_text/1.0.0/e3d5612f02fe5f11826a0d9614328b1772e27e5d685f4ec438e7f768e4581734...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/845M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49f6d24388f046479595e5ada1c5b722"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting num_proc from 60 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/708241 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0867edcfd0124830bdf35f9f12133bc9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cc_news downloaded and prepared to C:/Users/USER/.cache/huggingface/datasets/cc_news/plain_text/1.0.0/e3d5612f02fe5f11826a0d9614328b1772e27e5d685f4ec438e7f768e4581734. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "dataset_en: Dataset = load_dataset(\n",
    "    'cc_news',\n",
    "    split = \"train[:3%]\",\n",
    "    num_proc = min(8 * multiprocessing.cpu_count(), 60)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "_WKvnTbijURV",
    "ExecuteTime": {
     "end_time": "2023-09-12T15:18:40.899281700Z",
     "start_time": "2023-09-12T15:18:40.877738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example english passage: There's a surprising twist to Regina Willoughby's last season with Columbia City Ballet: It's also her 18-year-old daughter Melina's first season with the company. Regina, 40, will retire from the stage in March, just as her daughter starts her own career as a trainee. But for this one season, they're sharing the stage together.\n",
      "Performing Side-By-Side In The Nutcracker\n",
      "Regina and Melina are not only dancing in the same Nutcracker this month, they're onstage at the same time: Regina is doing Snow Queen, while Melina is in the snow corps, and they're both in the Arabian divertissement. \"It's very surreal to be dancing it together,\" says Regina. \"I don't know that I ever thought Melina would take ballet this far.\"\n",
      "Left: Regina and Melina with another company member post-snow scene in 2003. Right: The pair post-snow scene in 2017 (in the same theater)\n",
      "Keep reading at dancemagazine.com.\n"
     ]
    }
   ],
   "source": [
    "example_passage_en: str = dataset_en[0]['text']\n",
    "print(f\"example english passage: {example_passage_en}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhvTLZSvh40a"
   },
   "source": [
    "### Preparasi\n",
    "\n",
    "Anda diperbolehkan untuk melakukan instalasi packages dan download resources tambahan bila diperlukan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "oS0gijUM5mpP",
    "ExecuteTime": {
     "end_time": "2023-09-12T15:44:58.884016600Z",
     "start_time": "2023-09-12T15:44:51.047607400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (2.10.1)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: spacy in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (3.6.1)\n",
      "Requirement already satisfied: xxhash in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: multiprocess in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (0.15.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (2022.11.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (22.0)\n",
      "Requirement already satisfied: responses<0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: aiohttp in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (2.4.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (2.0.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (0.10.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (2.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (65.6.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.6.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from responses<0.19->datasets) (1.16.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (4.7.1)\n"
     ]
    }
   ],
   "source": [
    "# Install packages as needed\n",
    "!pip install datasets nltk spacy\n",
    "!pip install typing-extensions --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'deprecated' from 'typing_extensions' (C:\\ProgramData\\anaconda3\\lib\\site-packages\\typing_extensions.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\confection\\__init__.py:38\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 38\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpydantic\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv1\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseModel, Extra, ValidationError, create_model\n\u001B[0;32m     39\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpydantic\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv1\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfields\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ModelField\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pydantic\\__init__.py:13\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpydantic_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore_schema\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      5\u001B[0m     FieldSerializationInfo,\n\u001B[0;32m      6\u001B[0m     FieldValidationInfo,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     10\u001B[0m     ValidatorFunctionWrapHandler,\n\u001B[0;32m     11\u001B[0m )\n\u001B[1;32m---> 13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dataclasses\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_internal\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_annotated_handlers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     15\u001B[0m     GetCoreSchemaHandler \u001B[38;5;28;01mas\u001B[39;00m GetCoreSchemaHandler,\n\u001B[0;32m     16\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pydantic\\dataclasses.py:11\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping_extensions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Literal, dataclass_transform\n\u001B[1;32m---> 11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_internal\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _config, _decorators, _typing_extra\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_internal\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _dataclasses \u001B[38;5;28;01mas\u001B[39;00m _pydantic_dataclasses\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pydantic\\_internal\\_config.py:9\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping_extensions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Literal, Self\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ConfigDict, ExtraValues, JsonEncoder, JsonSchemaExtraCallable\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01merrors\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PydanticUserError\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pydantic\\config.py:9\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_migration\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m getattr_migration\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdeprecated\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseConfig\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdeprecated\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Extra \u001B[38;5;28;01mas\u001B[39;00m _Extra\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pydantic\\deprecated\\config.py:6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TYPE_CHECKING, Any\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping_extensions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Literal, deprecated\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_internal\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _config\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'deprecated' from 'typing_extensions' (C:\\ProgramData\\anaconda3\\lib\\site-packages\\typing_extensions.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[38], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mspacy\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\spacy\\__init__.py:6\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Any, Dict, Iterable, Union\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01merrors\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m setup_default_warnings\n\u001B[0;32m      8\u001B[0m setup_default_warnings()  \u001B[38;5;66;03m# noqa: E402\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# These are imported as part of the API\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\spacy\\errors.py:3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Literal\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mErrorsWithCodes\u001B[39;00m(\u001B[38;5;28mtype\u001B[39m):\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getattribute__\u001B[39m(\u001B[38;5;28mself\u001B[39m, code):\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\spacy\\compat.py:4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"Helpers for Python and platform compatibility.\"\"\"\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mthinc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m copy_array\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcPickle\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpickle\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\thinc\\__init__.py:5\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mabout\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __version__\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m registry\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# fmt: off\u001B[39;00m\n\u001B[0;32m      8\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mregistry\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__version__\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     11\u001B[0m ]\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\thinc\\config.py:2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcatalogue\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mconfection\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mconfection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m VARIABLE_RE, Config, ConfigValidationError, Promise\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtypes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Decorator\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\confection\\__init__.py:42\u001B[0m\n\u001B[0;32m     40\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpydantic\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv1\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmain\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ModelMetaclass\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[1;32m---> 42\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpydantic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseModel, create_model, ValidationError, Extra  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[0;32m     43\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpydantic\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmain\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ModelMetaclass  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[0;32m     44\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpydantic\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfields\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ModelField  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pydantic\\__init__.py:13\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpydantic_core\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpydantic_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore_schema\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      5\u001B[0m     FieldSerializationInfo,\n\u001B[0;32m      6\u001B[0m     FieldValidationInfo,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     10\u001B[0m     ValidatorFunctionWrapHandler,\n\u001B[0;32m     11\u001B[0m )\n\u001B[1;32m---> 13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dataclasses\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_internal\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_annotated_handlers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     15\u001B[0m     GetCoreSchemaHandler \u001B[38;5;28;01mas\u001B[39;00m GetCoreSchemaHandler,\n\u001B[0;32m     16\u001B[0m )\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_internal\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_annotated_handlers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     18\u001B[0m     GetJsonSchemaHandler \u001B[38;5;28;01mas\u001B[39;00m GetJsonSchemaHandler,\n\u001B[0;32m     19\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pydantic\\dataclasses.py:11\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TYPE_CHECKING, Any, Callable, Generic, NoReturn, TypeVar, overload\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping_extensions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Literal, dataclass_transform\n\u001B[1;32m---> 11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_internal\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _config, _decorators, _typing_extra\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_internal\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _dataclasses \u001B[38;5;28;01mas\u001B[39;00m _pydantic_dataclasses\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_migration\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m getattr_migration\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pydantic\\_internal\\_config.py:9\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpydantic_core\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m core_schema\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping_extensions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Literal, Self\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ConfigDict, ExtraValues, JsonEncoder, JsonSchemaExtraCallable\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01merrors\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PydanticUserError\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mwarnings\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PydanticDeprecatedSince20\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pydantic\\config.py:9\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping_extensions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Literal, TypeAlias, TypedDict\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_migration\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m getattr_migration\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdeprecated\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseConfig\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdeprecated\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Extra \u001B[38;5;28;01mas\u001B[39;00m _Extra\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m TYPE_CHECKING:\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pydantic\\deprecated\\config.py:6\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TYPE_CHECKING, Any\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping_extensions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Literal, deprecated\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_internal\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _config\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mwarnings\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PydanticDeprecatedSince20\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'deprecated' from 'typing_extensions' (C:\\ProgramData\\anaconda3\\lib\\site-packages\\typing_extensions.py)"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T16:04:11.239390700Z",
     "start_time": "2023-09-12T16:04:09.948170900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vby6_iOkiXKh"
   },
   "source": [
    "### 2.1: Tokenization\n",
    "\n",
    "Anda diminta melakukan tokenisasi text menjadi sekumpulan token sama seperti contoh pada Bagian 1.1 dengan ketentuan harus menggunakan regex. Tampilkan penerapan tokenisasi ini kepada contoh salah satu artikel (`example_passage_en`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CxcHn3fJ-33H"
   },
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I50k_CJljOu5"
   },
   "source": [
    "### 2.2: Lemmatization\n",
    "\n",
    "Anda diminta melakukan lematisasi untuk token-token yang sudah diproses sebelumnya dengan ketentuan harus menggunakan library [Spacy](https://spacy.io/). Tampilkan penerapan lematisasi ini terhadap hasil tokenisasi kepada contoh sebelumnya. Silahkan lakukan eksplorasi terhadap cara penggunaan tools ini, seperti pembentukan instance Doc, pengambilan lemma dari kata, dsb.\n",
    "\n",
    "Tips:\n",
    "- Anda dapat membentuk instance Doc (document) tanpa menggunakan tokenizer dari Spacy.\n",
    "- Anda dapat melakukan deaktivasi terhadap modul-modul diluar lemmatizer yang tidak dibutuhkan. Hal ini penting untuk peningkatan performa durasi eksekusi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_szU5y7EBgzD"
   },
   "outputs": [],
   "source": [
    "# Lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqmnk7wsmqYY"
   },
   "source": [
    "### 2.3: Stemming\n",
    "\n",
    "Anda diminta untuk melakukan stemming terhadap kumpulan token dari pemrosesan sebelumnya dengan ketentuan harus menggunakan library [NLTK](https://www.nltk.org/). NLTK memiliki beberapa jenis stemmer untuk bahasa Inggris, dan anda boleh memilih salah satunya. Tampilkan penerapan stemming ini terhadap hasil lematisasi pada contoh sebelumnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vGkIpGlDD3E"
   },
   "outputs": [],
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9urGt-HnR-C"
   },
   "source": [
    "### 2.4: Stop Words\n",
    "\n",
    "Anda diminta untuk mencari kumpulan stop words yang telah disediakan oleh library [NLTK](https://www.nltk.org/). Silahkan lakukan eksplorasi bagaimana cara mendapatkan kumpulan stop words tersebut dan tampilkan ke output untuk anda pelajari. Anda juga diminta untuk mengimplementasikan penghapusan stop words dari token-token yang telah diproses sebelumnya menggunakan kumpulan stop words tersebut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TBeD0S3B7-24"
   },
   "outputs": [],
   "source": [
    "# Stop Words Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKnKWkSBtcLf"
   },
   "source": [
    "### 2.5: Preprocess Data\n",
    "\n",
    "Menggunakan function-function yang telah anda buat dari eksplorasi diatas, silahkan rangkai function tokenisasi, lematisasi, dan stemming untuk diterapkan terhadap seluruh data dalam dataset bahasa Inggris yang sudah disediakan, serupa seperti Bagian 1. Khusus untuk tahap ini, anda tidak perlu melakukan stop words removal sebagai persiapan untuk soal selanjutnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZV0GXybm8cTd"
   },
   "outputs": [],
   "source": [
    "# Define your preprocessing pipeline as a function\n",
    "# Apply preprocess to all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbUedzWipQwr"
   },
   "source": [
    "### 2.6: Word Count\n",
    "\n",
    "Anda diminta untuk mengumpulkan seluruh token unik beserta dengan frekuensi kemunculannya dalam dataset ini, atau dengan kata lain anda diminta melakukan word counting. Hasil akhir yang diharapkan berbentuk list of tuple (string token, integer count).\n",
    "\n",
    "Sebagai contoh, bila terdapat artikel A, B, dan C dalam dataset, dimana masing-masing dokumen berisi:\n",
    "- Artikel A: \"computer\", \"science\"\n",
    "- Artikel B: \"computer\", \"program\"\n",
    "- Artikel C: \"program\", \"execution\"\n",
    "\n",
    "Maka, diharapkan output list of tuple (string, integer) berupa:\n",
    "- (computer, 2)\n",
    "- (program, 2)\n",
    "- (science, 1)\n",
    "- (execution, 1)\n",
    "\n",
    "Tips:\n",
    "- Anda dapat memanfaatkan fungsi map() dari dataset untuk menerapkan function kepada masing-masing row / artikel secara paralel.\n",
    "- Perhatikan struktur data yang anda gunakan sebagai output dari map(). Struktur data kompleks seperti dictionary dapat memperlambat pemrosesan. Anda dapat melakukan serialization (e.g. menjadi JSON string) bila menemukan isu terkait hal ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avH3OcJ5re6U"
   },
   "outputs": [],
   "source": [
    "# Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLvWlVDjsVhp"
   },
   "source": [
    "### 2.7: Gathering Your Own Stop Words\n",
    "\n",
    "Setelah anda mendapatkan kumpulan token-frekuensi dari proses sebelumnya, gunakan data tersebut untuk membentuk stop words anda sendiri. Ambil top 200 token yang memiliki kemunculan terbanyak.\n",
    "\n",
    "Kemudian, tampilkan stop words yang sudah anda kumpulkan dengan stop words dari NLTK. Bandingkan keduanya. Apa yang anda temukan dari komparasi tersebut? Token jenis seperti apa yang hanya ditemukan dari hasil pengumpulan anda?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LoB2l8p7rkq7"
   },
   "outputs": [],
   "source": [
    "# Take top 200 as Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EjkTkeHuX-_"
   },
   "source": [
    "### 2.8: Character Count\n",
    "\n",
    "Selanjutnya, mirip dengan word counting, anda diminta untuk mengumpulkan karakter unik dari seluruh token-token dalam dataset beserta dengan frekuensi kemunculannya (character counting).\n",
    "\n",
    "Sebagai contoh, bila terdapat artikel A dan B dalam dataset, dimana masing-masing dokumen berisi:\n",
    "- Artikel A: \"computer\", \"science\"\n",
    "- Artikel B: \"computer\", \"program\"\n",
    "\n",
    "Maka, diharapkan output list of tuple (string, integer) berupa:\n",
    "- ('c', 4)\n",
    "- ('e', 4)\n",
    "- ('r', 4)\n",
    "- ...\n",
    "\n",
    "Ketentuan:\n",
    "- Untuk simplifikasi, anda hanya perlu menghitung karakter alphabet (a-z). Silahkan hapus karakter lain (e.g. tanda baca)\n",
    "- Untuk simplifikasi, anda dapat mengubah huruf kapital dalam seluruh token menjadi huruf kecil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fCR6XNI7uYk7"
   },
   "outputs": [],
   "source": [
    "# Character Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWmiUlIgxLOR"
   },
   "source": [
    "### 2.9: Comparing Character Count between English and Indonesian\n",
    "\n",
    "Kumpulkan juga character count untuk dataset bahasa Indonesia pada Bagian 1. Tampilkan kedua data tersebut dan bandingkan. Apa karakter yang paling dominan dalam bahasa Indonesia? Apa karakter yang paling dominan dalam bahasa Inggris?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v8o5fshfxc0H"
   },
   "outputs": [],
   "source": [
    "# Character-count for English\n",
    "# Compare character count between English & Indonesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2k9zrW0hZAAj"
   },
   "outputs": [],
   "source": [
    "end_time_en: int = time.time()\n",
    "duration_sec_en: int = end_time_en - start_time_en\n",
    "duration_min_en: float = round(duration_sec_en / 60, 2)\n",
    "! date\n",
    "print(f\"Bagian 2: done in {duration_min_en} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Demikian tugas eksplorasi pemrograman 1. Untuk pengumpulan tugas, silahkan mengikuti petunjuk dalam dokumen soal."
   ],
   "metadata": {
    "id": "Uk31glbBaTfF"
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
